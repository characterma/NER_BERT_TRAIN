{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%autoreload utils.tag_char\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "\n",
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "from tqdm import tqdm_pandas\n",
    "from utils import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'date': '0530',\n",
       " 'experiment_name': 'bert_p=0.5_pos=sentence',\n",
       " 'mode': 'BP',\n",
       " 'entity_type': ['Company', 'Person'],\n",
       " 'num_labels': 5,\n",
       " 'threshold': 0.2,\n",
       " 'train_data': '/train_dataset',\n",
       " 'valid_data': '/valid_dataset',\n",
       " 'test_data': '/test_dataset',\n",
       " 'pretrained_model': 'hfl/chinese-roberta-wwm-ext',\n",
       " 'do_training': False,\n",
       " 'max_len': 512,\n",
       " 'batch_size': 32,\n",
       " 'n_epochs': 5,\n",
       " 'lr': '1e-5',\n",
       " 'do_prediction': True}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../config.yaml\", 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=32, date='0530', do_prediction=True, do_training=False, entity_type=['Company', 'Person'], experiment_name='bert_p=0.5_pos=sentence', lr='1e-5', max_len=512, mode='BP', n_epochs=5, num_labels=5, prefix_path='../experiments/0530_bert_p=0.5_pos=sentence_BP', pretrained_model='hfl/chinese-roberta-wwm-ext', test_data='/test_dataset', threshold=0.2, train_data='/train_dataset', valid_data='/valid_dataset')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description='') \n",
    "\n",
    "parser.set_defaults(**config)\n",
    "args = parser.parse_args(args = [])\n",
    "\n",
    "parser.add_argument(\"--prefix_path\", type=str, default=f\"../experiments/{args.date}_{args.experiment_name}_{args.mode}\")\n",
    "args = parser.parse_args(args = [])\n",
    "\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'O': 0, 'B-Company': 1, 'I-Company': 2, 'B-Person': 3, 'I-Person': 4}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_ids, ids_to_labels = define_labels(param_args=args)\n",
    "labels_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立实验文件夹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.makedirs(args.prefix_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## split to train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32254, 15)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>ner</th>\n",
       "      <th>matched_keywords</th>\n",
       "      <th>include_company</th>\n",
       "      <th>include_person</th>\n",
       "      <th>split_sentence_index</th>\n",
       "      <th>Companys</th>\n",
       "      <th>Persons</th>\n",
       "      <th>context_cleaned</th>\n",
       "      <th>person_matched</th>\n",
       "      <th>company_matched</th>\n",
       "      <th>context_keywords</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023063021774796465&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '寶申...</td>\n",
       "      <td>['宝申控股']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['宝申控股']</td>\n",
       "      <td>[]</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['宝申控股']</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。||可能的实体：宝申控股||宝申...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023063021774796485&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '利駿...</td>\n",
       "      <td>['利骏集团香港']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['利骏集团', '利骏集团香港']</td>\n",
       "      <td>[]</td>\n",
       "      <td>利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...</td>\n",
       "      <td>[]</td>\n",
       "      <td>['利骏集团香港']</td>\n",
       "      <td>利骏集团香港(08360) 股东特别大会的结果。||可能的实体：利骏集团香港||利骏集团香港...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023063021774796605&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '21...</td>\n",
       "      <td>['21世纪经济报道', '东南大学', '杨学鹏', '吴刚', '胡广杰', '黄如',...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['21世纪经济报道', '东南大学']</td>\n",
       "      <td>['杨学鹏', '吴刚', '胡广杰', '黄如', '郭晨', '陈之常', '徐光辉']</td>\n",
       "      <td>推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...</td>\n",
       "      <td>['杨学鹏', '吴刚', '胡广杰', '黄如', '郭晨', '陈之常', '徐光辉']</td>\n",
       "      <td>['21世纪经济报道', '东南大学']</td>\n",
       "      <td>推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023063021774796605&amp;&amp;1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>他说,集成电路产业是我省具有较强竞争力的优势产业之一,已形成覆盖设计、制造、封测、设备、材料...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '21...</td>\n",
       "      <td>['东南大学', '胡广杰', '陈之常']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>['东南大学']</td>\n",
       "      <td>['陈之常', '胡广杰']</td>\n",
       "      <td>他说,集成电路产业是我省具有较强竞争力的优势产业之一,已形成覆盖设计、制造、封测、设备、材料...</td>\n",
       "      <td>['胡广杰', '陈之常']</td>\n",
       "      <td>['东南大学']</td>\n",
       "      <td>他说,集成电路产业是我省具有较强竞争力的优势产业之一,已形成覆盖设计、制造、封测、设备、材料...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    docid  headline  \\\n",
       "0  2023063021774796465&&0       NaN   \n",
       "1  2023063021774796485&&0       NaN   \n",
       "2  2023063021774796605&&0       NaN   \n",
       "3  2023063021774796605&&1       NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...   \n",
       "1  利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...   \n",
       "2  推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...   \n",
       "3  他说,集成电路产业是我省具有较强竞争力的优势产业之一,已形成覆盖设计、制造、封测、设备、材料...   \n",
       "\n",
       "                                                 ner  \\\n",
       "0  [{'label_name': 'Company', 'text_segment': '寶申...   \n",
       "1  [{'label_name': 'Company', 'text_segment': '利駿...   \n",
       "2  [{'label_name': 'Company', 'text_segment': '21...   \n",
       "3  [{'label_name': 'Company', 'text_segment': '21...   \n",
       "\n",
       "                                    matched_keywords  include_company  \\\n",
       "0                                           ['宝申控股']             True   \n",
       "1                                         ['利骏集团香港']             True   \n",
       "2  ['21世纪经济报道', '东南大学', '杨学鹏', '吴刚', '胡广杰', '黄如',...             True   \n",
       "3                             ['东南大学', '胡广杰', '陈之常']             True   \n",
       "\n",
       "   include_person  split_sentence_index              Companys  \\\n",
       "0            True                     0              ['宝申控股']   \n",
       "1            True                     0    ['利骏集团', '利骏集团香港']   \n",
       "2            True                     0  ['21世纪经济报道', '东南大学']   \n",
       "3            True                     1              ['东南大学']   \n",
       "\n",
       "                                          Persons  \\\n",
       "0                                              []   \n",
       "1                                              []   \n",
       "2  ['杨学鹏', '吴刚', '胡广杰', '黄如', '郭晨', '陈之常', '徐光辉']   \n",
       "3                                  ['陈之常', '胡广杰']   \n",
       "\n",
       "                                     context_cleaned  \\\n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...   \n",
       "1  利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...   \n",
       "2  推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...   \n",
       "3  他说,集成电路产业是我省具有较强竞争力的优势产业之一,已形成覆盖设计、制造、封测、设备、材料...   \n",
       "\n",
       "                                   person_matched       company_matched  \\\n",
       "0                                              []              ['宝申控股']   \n",
       "1                                              []            ['利骏集团香港']   \n",
       "2  ['杨学鹏', '吴刚', '胡广杰', '黄如', '郭晨', '陈之常', '徐光辉']  ['21世纪经济报道', '东南大学']   \n",
       "3                                  ['胡广杰', '陈之常']              ['东南大学']   \n",
       "\n",
       "                                    context_keywords   sign  \n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。||可能的实体：宝申控股||宝申...  False  \n",
       "1  利骏集团香港(08360) 股东特别大会的结果。||可能的实体：利骏集团香港||利骏集团香港...  False  \n",
       "2  推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...  False  \n",
       "3  他说,集成电路产业是我省具有较强竞争力的优势产业之一,已形成覆盖设计、制造、封测、设备、材料...  False  "
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_data = pd.read_pickle(\"../data/data_starbucks_yihetang_aligned.pkl\")\n",
    "df_data = pd.read_excel(\"../bochk/manual_label_data.xlsx\")\n",
    "print(df_data.shape)\n",
    "df_data.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_data['person_matched'] = df_data['person_matched'].map(eval)\n",
    "df_data['company_matched'] = df_data['company_matched'].map(eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 15)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['sign'] = df_data.apply(lambda row: row['company_matched'].__len__() > 0 or row['person_matched'].__len__() > 0, axis=1)\n",
    "df_data[df_data['sign']==False].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_data['docid'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25261, 3283, 3710)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = Dataset.from_pandas(df_data[[\"docid\"]])\n",
    "\n",
    "splitted_dataset = dataset.train_test_split(train_size=0.885, seed=109)\n",
    "train_ = splitted_dataset['train']\n",
    "split_ = train_.train_test_split(train_size=0.885, seed=109)\n",
    "\n",
    "train_set = split_['train']\n",
    "valid_set = split_['test']\n",
    "test_set = splitted_dataset['test']\n",
    "\n",
    "len(train_set), len(valid_set), len(test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(f\"{args.prefix_path}/train_docid.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(train_set['docid']))\n",
    "\n",
    "with open(f\"{args.prefix_path}/valid_docid.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(valid_set['docid']))\n",
    "\n",
    "with open(f\"{args.prefix_path}/test_docid.txt\", \"w\") as f:\n",
    "    f.write(\"\\n\".join(test_set['docid']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read train, valid, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>ner</th>\n",
       "      <th>matched_keywords</th>\n",
       "      <th>include_company</th>\n",
       "      <th>include_person</th>\n",
       "      <th>split_sentence_index</th>\n",
       "      <th>Companys</th>\n",
       "      <th>Persons</th>\n",
       "      <th>context_cleaned</th>\n",
       "      <th>person_matched</th>\n",
       "      <th>company_matched</th>\n",
       "      <th>context_keywords</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023063021774796465&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '寶申...</td>\n",
       "      <td>['宝申控股']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['宝申控股']</td>\n",
       "      <td>[]</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[宝申控股]</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。||可能的实体：宝申控股||宝申...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    docid  headline  \\\n",
       "0  2023063021774796465&&0       NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...   \n",
       "\n",
       "                                                 ner matched_keywords  \\\n",
       "0  [{'label_name': 'Company', 'text_segment': '寶申...         ['宝申控股']   \n",
       "\n",
       "   include_company  include_person  split_sentence_index  Companys Persons  \\\n",
       "0             True            True                     0  ['宝申控股']      []   \n",
       "\n",
       "                                     context_cleaned person_matched  \\\n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...             []   \n",
       "\n",
       "  company_matched                                   context_keywords   sign  \n",
       "0          [宝申控股]  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。||可能的实体：宝申控股||宝申...  False  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "df_all = pd.read_excel(\"../bochk/manual_label_data.xlsx\")\n",
    "df_all['person_matched'] = df_all['person_matched'].map(eval)\n",
    "df_all['company_matched'] = df_all['company_matched'].map(eval)\n",
    "df_all['content'] = df_all['content'].astype(str)\n",
    "df_all.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>ner</th>\n",
       "      <th>matched_keywords</th>\n",
       "      <th>include_company</th>\n",
       "      <th>include_person</th>\n",
       "      <th>split_sentence_index</th>\n",
       "      <th>Companys</th>\n",
       "      <th>Persons</th>\n",
       "      <th>context_cleaned</th>\n",
       "      <th>person_matched</th>\n",
       "      <th>company_matched</th>\n",
       "      <th>context_keywords</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32249</th>\n",
       "      <td>997&amp;&amp;1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>针对中央环境保护督察指出的海域岸线自然生态和风貌破坏等问题，海南一是着力解决填海造地破坏海洋...</td>\n",
       "      <td>[{'label_name': 'Time', 'text_segment': '2019年...</td>\n",
       "      <td>['毛超峰']</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>[]</td>\n",
       "      <td>['毛超峰']</td>\n",
       "      <td>针对中央环境保护督察指出的海域岸线自然生态和风貌破坏等问题，海南一是着力解决填海造地破坏海洋...</td>\n",
       "      <td>[毛超峰]</td>\n",
       "      <td>[]</td>\n",
       "      <td>针对中央环境保护督察指出的海域岸线自然生态和风貌破坏等问题，海南一是着力解决填海造地破坏海洋...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32250</th>\n",
       "      <td>998&amp;&amp;1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>无独有偶。博州人才政策对于住房方面的关注在其他三四五线城市动作中也有所体现。比如，4月22日...</td>\n",
       "      <td>[{'label_name': 'Time', 'text_segment': '二十一世紀...</td>\n",
       "      <td>['国际金融报', '易居研究院智库中心', '严跃进']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>['国际金融报', '易居', '易居研究院', '易居研究院智库中心']</td>\n",
       "      <td>['严跃进']</td>\n",
       "      <td>无独有偶。博州人才政策对于住房方面的关注在其他三四五线城市动作中也有所体现。比如，4月22日...</td>\n",
       "      <td>[严跃进]</td>\n",
       "      <td>[国际金融报, 易居研究院智库中心]</td>\n",
       "      <td>无独有偶。博州人才政策对于住房方面的关注在其他三四五线城市动作中也有所体现。比如，4月22日...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32251</th>\n",
       "      <td>998&amp;&amp;2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...</td>\n",
       "      <td>[{'label_name': 'Time', 'text_segment': '二十一世紀...</td>\n",
       "      <td>['国际金融报', '严跃进']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>['国际金融报']</td>\n",
       "      <td>['严跃进']</td>\n",
       "      <td>“战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...</td>\n",
       "      <td>[严跃进]</td>\n",
       "      <td>[国际金融报]</td>\n",
       "      <td>“战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32252</th>\n",
       "      <td>998&amp;&amp;3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“西安就是一个例子。”沈路(化名)是西安本地人，刚回国就业时正好赶上抢人期。2017年3月，...</td>\n",
       "      <td>[{'label_name': 'Time', 'text_segment': '二十一世紀...</td>\n",
       "      <td>['易居研究院', '沈昕', '沈路']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>['易居', '易居研究院']</td>\n",
       "      <td>['沈昕', '沈路']</td>\n",
       "      <td>“西安就是一个例子。”沈路(化名)是西安本地人，刚回国就业时正好赶上抢人期。2017年3月，...</td>\n",
       "      <td>[沈昕, 沈路]</td>\n",
       "      <td>[易居研究院]</td>\n",
       "      <td>“西安就是一个例子。”沈路(化名)是西安本地人，刚回国就业时正好赶上抢人期。||可能的实体：...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32253</th>\n",
       "      <td>998&amp;&amp;4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>来自四川自贡富顺县的刘民(化名)目前选择在江苏常州创业，他对记者表示，“虽然我家乡很多人外出...</td>\n",
       "      <td>[{'label_name': 'Time', 'text_segment': '二十一世紀...</td>\n",
       "      <td>['国际金融报', '搜狐', '刘民', '刘明', '陈晨']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "      <td>['国际金融报', '搜狐']</td>\n",
       "      <td>['刘民', '刘明', '陈晨']</td>\n",
       "      <td>来自四川自贡富顺县的刘民(化名)目前选择在江苏常州创业，他对记者表示，“虽然我家乡很多人外出...</td>\n",
       "      <td>[刘民, 刘明, 陈晨]</td>\n",
       "      <td>[国际金融报, 搜狐]</td>\n",
       "      <td>来自四川自贡富顺县的刘民(化名)目前选择在江苏常州创业，他对记者表示，“虽然我家乡很多人外出...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        docid  headline                                            content  \\\n",
       "32249  997&&1       NaN  针对中央环境保护督察指出的海域岸线自然生态和风貌破坏等问题，海南一是着力解决填海造地破坏海洋...   \n",
       "32250  998&&1       NaN  无独有偶。博州人才政策对于住房方面的关注在其他三四五线城市动作中也有所体现。比如，4月22日...   \n",
       "32251  998&&2       NaN  “战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...   \n",
       "32252  998&&3       NaN  “西安就是一个例子。”沈路(化名)是西安本地人，刚回国就业时正好赶上抢人期。2017年3月，...   \n",
       "32253  998&&4       NaN  来自四川自贡富顺县的刘民(化名)目前选择在江苏常州创业，他对记者表示，“虽然我家乡很多人外出...   \n",
       "\n",
       "                                                     ner  \\\n",
       "32249  [{'label_name': 'Time', 'text_segment': '2019年...   \n",
       "32250  [{'label_name': 'Time', 'text_segment': '二十一世紀...   \n",
       "32251  [{'label_name': 'Time', 'text_segment': '二十一世紀...   \n",
       "32252  [{'label_name': 'Time', 'text_segment': '二十一世紀...   \n",
       "32253  [{'label_name': 'Time', 'text_segment': '二十一世紀...   \n",
       "\n",
       "                        matched_keywords  include_company  include_person  \\\n",
       "32249                            ['毛超峰']            False            True   \n",
       "32250      ['国际金融报', '易居研究院智库中心', '严跃进']             True            True   \n",
       "32251                   ['国际金融报', '严跃进']             True            True   \n",
       "32252              ['易居研究院', '沈昕', '沈路']             True            True   \n",
       "32253  ['国际金融报', '搜狐', '刘民', '刘明', '陈晨']             True            True   \n",
       "\n",
       "       split_sentence_index                               Companys  \\\n",
       "32249                     1                                     []   \n",
       "32250                     1  ['国际金融报', '易居', '易居研究院', '易居研究院智库中心']   \n",
       "32251                     2                              ['国际金融报']   \n",
       "32252                     3                        ['易居', '易居研究院']   \n",
       "32253                     4                        ['国际金融报', '搜狐']   \n",
       "\n",
       "                  Persons                                    context_cleaned  \\\n",
       "32249             ['毛超峰']  针对中央环境保护督察指出的海域岸线自然生态和风貌破坏等问题，海南一是着力解决填海造地破坏海洋...   \n",
       "32250             ['严跃进']  无独有偶。博州人才政策对于住房方面的关注在其他三四五线城市动作中也有所体现。比如，4月22日...   \n",
       "32251             ['严跃进']  “战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...   \n",
       "32252        ['沈昕', '沈路']  “西安就是一个例子。”沈路(化名)是西安本地人，刚回国就业时正好赶上抢人期。2017年3月，...   \n",
       "32253  ['刘民', '刘明', '陈晨']  来自四川自贡富顺县的刘民(化名)目前选择在江苏常州创业，他对记者表示，“虽然我家乡很多人外出...   \n",
       "\n",
       "      person_matched     company_matched  \\\n",
       "32249          [毛超峰]                  []   \n",
       "32250          [严跃进]  [国际金融报, 易居研究院智库中心]   \n",
       "32251          [严跃进]             [国际金融报]   \n",
       "32252       [沈昕, 沈路]             [易居研究院]   \n",
       "32253   [刘民, 刘明, 陈晨]         [国际金融报, 搜狐]   \n",
       "\n",
       "                                        context_keywords   sign  \n",
       "32249  针对中央环境保护督察指出的海域岸线自然生态和风貌破坏等问题，海南一是着力解决填海造地破坏海洋...  False  \n",
       "32250  无独有偶。博州人才政策对于住房方面的关注在其他三四五线城市动作中也有所体现。比如，4月22日...  False  \n",
       "32251  “战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...  False  \n",
       "32252  “西安就是一个例子。”沈路(化名)是西安本地人，刚回国就业时正好赶上抢人期。||可能的实体：...  False  \n",
       "32253  来自四川自贡富顺县的刘民(化名)目前选择在江苏常州创业，他对记者表示，“虽然我家乡很多人外出...  False  "
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>ner</th>\n",
       "      <th>matched_keywords</th>\n",
       "      <th>include_company</th>\n",
       "      <th>include_person</th>\n",
       "      <th>split_sentence_index</th>\n",
       "      <th>Companys</th>\n",
       "      <th>Persons</th>\n",
       "      <th>context_cleaned</th>\n",
       "      <th>person_matched</th>\n",
       "      <th>company_matched</th>\n",
       "      <th>context_keywords</th>\n",
       "      <th>sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>32251</th>\n",
       "      <td>998&amp;&amp;2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>“战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...</td>\n",
       "      <td>[{'label_name': 'Time', 'text_segment': '二十一世紀...</td>\n",
       "      <td>['国际金融报', '严跃进']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "      <td>['国际金融报']</td>\n",
       "      <td>['严跃进']</td>\n",
       "      <td>“战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...</td>\n",
       "      <td>[严跃进]</td>\n",
       "      <td>[国际金融报]</td>\n",
       "      <td>“战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        docid  headline                                            content  \\\n",
       "32251  998&&2       NaN  “战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...   \n",
       "\n",
       "                                                     ner  matched_keywords  \\\n",
       "32251  [{'label_name': 'Time', 'text_segment': '二十一世紀...  ['国际金融报', '严跃进']   \n",
       "\n",
       "       include_company  include_person  split_sentence_index   Companys  \\\n",
       "32251             True            True                     2  ['国际金融报']   \n",
       "\n",
       "       Persons                                    context_cleaned  \\\n",
       "32251  ['严跃进']  “战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...   \n",
       "\n",
       "      person_matched company_matched  \\\n",
       "32251          [严跃进]         [国际金融报]   \n",
       "\n",
       "                                        context_keywords   sign  \n",
       "32251  “战火”已悄悄弥漫至区、县等更小的行政单位。不过，一位三线城市人才申报工作者对记者表示，对于...  False  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[df_all['docid'] == '998&&2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all['docid'].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25261, 3283, 3710)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(f\"{args.prefix_path}/train_docid.txt\", \"r\") as f:\n",
    "    train_docid = f.read().split(\"\\n\")\n",
    "\n",
    "with open(f\"{args.prefix_path}/valid_docid.txt\", \"r\") as f:\n",
    "    valid_docid = f.read().split(\"\\n\")\n",
    "\n",
    "with open(f\"{args.prefix_path}/test_docid.txt\", \"r\") as f:\n",
    "    test_docid = f.read().split(\"\\n\")\n",
    "\n",
    "len(train_docid), len(valid_docid), len(test_docid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25261, 3283, 3710)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_all[df_all[\"docid\"].isin(train_docid)].reset_index(drop=True)\n",
    "df_valid = df_all[df_all[\"docid\"].isin(valid_docid)].reset_index(drop=True)\n",
    "df_test = df_all[df_all[\"docid\"].isin(test_docid)].reset_index(drop=True)\n",
    "\n",
    "len(df_train), len(df_valid), len(df_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train, valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.threshold == 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tqdm.pandas()\n",
    "df_train[\"input_text\"] = df_train.apply(lambda x: x[\"context_keywords\"] if np.random.random()>args.threshold else x[\"context_cleaned\"], axis = 1)\n",
    "df_valid[\"input_text\"] = df_valid.apply(lambda x: x[\"context_keywords\"] if np.random.random()>args.threshold else x[\"context_cleaned\"], axis = 1)\n",
    "\n",
    "df_train['input_text'] = df_train['context_cleaned']\n",
    "df_valid['input_text'] = df_valid['context_cleaned']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'希拉里参加下一任美国总统竞选。美国前国务卿希拉里。克林顿12号正式宣布,参选2016年、下届美国总统选举,表示希望成为普罗美国民众的斗士。希拉里透过官方竞选网站宣布参选,视频表达的要求,触及社会各个社群。这是希拉里继2008年,民主党内初选败于奥巴马后,第二次角逐总统选举提名,在网站发布参选视频前,她的竞选助手,已先向支持者发出电邮。预告她在不久后,将前往初选最早投票的州份,包括艾奥瓦和新罕布什尔州,展开竞选演说等活动,竞选主轴将着重社会经济不平等,以及她致力成为美国首位女总统的历史意义。希拉里:我要竞逐美国总统,美国人正努力从经济困难期恢复过来,但好处仍在向上层阶级倾斜,普罗美国民众需要一个斗士,我希望成为这个斗士。希拉里在美国政坛曾经扮演过许多不同角色,包括第一夫人、参议员和国务卿。现在她将作第二次尝试,希望成为美国首位女总统。现年67岁的希拉里,毫不掩饰希望入主白宫的愿望。希拉里也说,自己可胜任总统。希拉里:我认为这样说才公平,难道你不想有一天看到美国有位女总统。普罗美国民众需要一个斗士,我希望成为这个斗士。希拉里:我确实有独特优势及经历,知道什么便利美国运作,什么阻碍美国运作,也清楚总统可以做什么,总统应做什么'"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[5556, 'input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月三十日举行的股东特别大会投票结果(112kb, pdf)'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.loc[1, 'input_text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['docid', 'headline', 'content', 'ner', 'matched_keywords',\n",
       "       'include_company', 'include_person', 'split_sentence_index', 'Companys',\n",
       "       'Persons', 'context_cleaned', 'person_matched', 'company_matched',\n",
       "       'context_keywords', 'sign', 'input_text'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tag char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def tag_char(df, args):\n",
    "    def _tag_char(example, param_args):\n",
    "        content = example['input_text'] if pd.notna(example['input_text']) else \"\"\n",
    "        try:\n",
    "            tag = ['O'] * len(str(content))\n",
    "\n",
    "            for entity_type in param_args.entity_type:\n",
    "                pos_list = []\n",
    "                entity_list = example[f\"{entity_type}_matched\".lower()]\n",
    "                if entity_list == []:\n",
    "                    continue\n",
    "                else:\n",
    "                    for entity in entity_list:\n",
    "                        # try:\n",
    "                        #     pos_list.extend([(match.start(), match.end()) for match in re.finditer(entity, content)])\n",
    "                        # except Exception as e:\n",
    "                        #     print(entity, content)\n",
    "                        #     continue\n",
    "                        try:\n",
    "                            pos_list.extend([(match.start(), match.end()) for match in re.finditer(entity, content)])\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "                    for (start, end) in pos_list:\n",
    "                        tag[start] = f\"B-{entity_type}\"\n",
    "                        tag[start+1:end] = [f\"I-{entity_type}\"] * (end - start - 1)\n",
    "\n",
    "            assert len(content) == len(tag)\n",
    "            return tag\n",
    "        except:\n",
    "            return None\n",
    "    \n",
    "    tqdm.pandas(desc='tagging char-level label')\n",
    "    df['tag_char'] = df.apply(_tag_char, param_args=args, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "missing ), unterminated subpattern at position 13\n",
      "missing ), unterminated subpattern at position 1\n",
      "expected string or bytes-like object\n",
      "nothing to repeat at position 0\n",
      "unbalanced parenthesis at position 2\n",
      "nothing to repeat at position 0\n",
      "unbalanced parenthesis at position 10\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 3\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "nothing to repeat at position 0\n",
      "nothing to repeat at position 0\n",
      "nothing to repeat at position 0\n",
      "nothing to repeat at position 0\n",
      "nothing to repeat at position 0\n",
      "missing ), unterminated subpattern at position 4\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "unbalanced parenthesis at position 16\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 9\n",
      "expected string or bytes-like object\n",
      "missing ), unterminated subpattern at position 0\n",
      "unbalanced parenthesis at position 12\n",
      "missing ), unterminated subpattern at position 0\n",
      "missing ), unterminated subpattern at position 12\n",
      "multiple repeat at position 5\n",
      "missing ), unterminated subpattern at position 0\n",
      "nothing to repeat at position 0\n",
      "nothing to repeat at position 0\n",
      "nothing to repeat at position 0\n"
     ]
    }
   ],
   "source": [
    "df_train = tag_char(df_train, args)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_train[df_train['docid']=='995&&2'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unterminated character set at position 0\n",
      "nothing to repeat at position 0\n",
      "unbalanced parenthesis at position 4\n",
      "missing ), unterminated subpattern at position 8\n",
      "missing ), unterminated subpattern at position 8\n"
     ]
    }
   ],
   "source": [
    "df_valid = tag_char(df_valid, args)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize and align label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25261, 17)\n",
      "(25258, 17)\n",
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c642209f7224f8f9ad5a468d4e040a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=790), Label(value='0 / 790'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>ner</th>\n",
       "      <th>matched_keywords</th>\n",
       "      <th>include_company</th>\n",
       "      <th>include_person</th>\n",
       "      <th>split_sentence_index</th>\n",
       "      <th>Companys</th>\n",
       "      <th>Persons</th>\n",
       "      <th>context_cleaned</th>\n",
       "      <th>person_matched</th>\n",
       "      <th>company_matched</th>\n",
       "      <th>context_keywords</th>\n",
       "      <th>sign</th>\n",
       "      <th>input_text</th>\n",
       "      <th>tag_char</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>token_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023063021774796465&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '寶申...</td>\n",
       "      <td>['宝申控股']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['宝申控股']</td>\n",
       "      <td>[]</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[宝申控股]</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。||可能的实体：宝申控股||宝申...</td>\n",
       "      <td>False</td>\n",
       "      <td>宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...</td>\n",
       "      <td>[B-Company, I-Company, I-Company, I-Company, O...</td>\n",
       "      <td>[宝, 申, 控, 股, (, 0, 8, 1, 5, 1, ), 停, 牌, /, 内, ...</td>\n",
       "      <td>[B-Company, I-Company, I-Company, I-Company, O...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023063021774796485&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '利駿...</td>\n",
       "      <td>['利骏集团香港']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['利骏集团', '利骏集团香港']</td>\n",
       "      <td>[]</td>\n",
       "      <td>利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[利骏集团香港]</td>\n",
       "      <td>利骏集团香港(08360) 股东特别大会的结果。||可能的实体：利骏集团香港||利骏集团香港...</td>\n",
       "      <td>False</td>\n",
       "      <td>利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...</td>\n",
       "      <td>[B-Company, I-Company, I-Company, I-Company, I...</td>\n",
       "      <td>[利, 骏, 集, 团, 香, 港, (, 0, 8, 3, 6, 0, ), 股, 东, ...</td>\n",
       "      <td>[B-Company, I-Company, I-Company, I-Company, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023063021774796605&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '21...</td>\n",
       "      <td>['21世纪经济报道', '东南大学', '杨学鹏', '吴刚', '胡广杰', '黄如',...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['21世纪经济报道', '东南大学']</td>\n",
       "      <td>['杨学鹏', '吴刚', '胡广杰', '黄如', '郭晨', '陈之常', '徐光辉']</td>\n",
       "      <td>推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...</td>\n",
       "      <td>[杨学鹏, 吴刚, 胡广杰, 黄如, 郭晨, 陈之常, 徐光辉]</td>\n",
       "      <td>[21世纪经济报道, 东南大学]</td>\n",
       "      <td>推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...</td>\n",
       "      <td>False</td>\n",
       "      <td>推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[推, 进, 科, 技, 自, 立, 自, 强, ，, 国, 家, 集, 成, 电, 路, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    docid  headline  \\\n",
       "0  2023063021774796465&&0       NaN   \n",
       "1  2023063021774796485&&0       NaN   \n",
       "2  2023063021774796605&&0       NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...   \n",
       "1  利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...   \n",
       "2  推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...   \n",
       "\n",
       "                                                 ner  \\\n",
       "0  [{'label_name': 'Company', 'text_segment': '寶申...   \n",
       "1  [{'label_name': 'Company', 'text_segment': '利駿...   \n",
       "2  [{'label_name': 'Company', 'text_segment': '21...   \n",
       "\n",
       "                                    matched_keywords  include_company  \\\n",
       "0                                           ['宝申控股']             True   \n",
       "1                                         ['利骏集团香港']             True   \n",
       "2  ['21世纪经济报道', '东南大学', '杨学鹏', '吴刚', '胡广杰', '黄如',...             True   \n",
       "\n",
       "   include_person  split_sentence_index              Companys  \\\n",
       "0            True                     0              ['宝申控股']   \n",
       "1            True                     0    ['利骏集团', '利骏集团香港']   \n",
       "2            True                     0  ['21世纪经济报道', '东南大学']   \n",
       "\n",
       "                                          Persons  \\\n",
       "0                                              []   \n",
       "1                                              []   \n",
       "2  ['杨学鹏', '吴刚', '胡广杰', '黄如', '郭晨', '陈之常', '徐光辉']   \n",
       "\n",
       "                                     context_cleaned  \\\n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...   \n",
       "1  利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...   \n",
       "2  推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...   \n",
       "\n",
       "                     person_matched   company_matched  \\\n",
       "0                                []            [宝申控股]   \n",
       "1                                []          [利骏集团香港]   \n",
       "2  [杨学鹏, 吴刚, 胡广杰, 黄如, 郭晨, 陈之常, 徐光辉]  [21世纪经济报道, 东南大学]   \n",
       "\n",
       "                                    context_keywords   sign  \\\n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。||可能的实体：宝申控股||宝申...  False   \n",
       "1  利骏集团香港(08360) 股东特别大会的结果。||可能的实体：利骏集团香港||利骏集团香港...  False   \n",
       "2  推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...  False   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌...   \n",
       "1  利骏集团香港(08360) 股东特别大会的结果。利骏集团香港(08360) 于二零二三年六月...   \n",
       "2  推进科技自立自强， 国家集成电路设计自动化技术创新中心正式揭牌。21世纪经济报道记者 郭晨 ...   \n",
       "\n",
       "                                            tag_char  \\\n",
       "0  [B-Company, I-Company, I-Company, I-Company, O...   \n",
       "1  [B-Company, I-Company, I-Company, I-Company, I...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                   tokenized_content  \\\n",
       "0  [宝, 申, 控, 股, (, 0, 8, 1, 5, 1, ), 停, 牌, /, 内, ...   \n",
       "1  [利, 骏, 集, 团, 香, 港, (, 0, 8, 3, 6, 0, ), 股, 东, ...   \n",
       "2  [推, 进, 科, 技, 自, 立, 自, 强, ，, 国, 家, 集, 成, 电, 路, ...   \n",
       "\n",
       "                                        token_labels  \n",
       "0  [B-Company, I-Company, I-Company, I-Company, O...  \n",
       "1  [B-Company, I-Company, I-Company, I-Company, I...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_train.shape)\n",
    "df_train = df_train[~df_train['tag_char'].isna()]\n",
    "print(df_train.shape)\n",
    "df_train = tokenize_and_align_labels(df_train, args)\n",
    "df_train.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3283, 17)\n",
      "(3283, 17)\n",
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d8734c4c2cf42ab82eca833a6f1e4d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=103), Label(value='0 / 103'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>ner</th>\n",
       "      <th>matched_keywords</th>\n",
       "      <th>include_company</th>\n",
       "      <th>include_person</th>\n",
       "      <th>split_sentence_index</th>\n",
       "      <th>Companys</th>\n",
       "      <th>Persons</th>\n",
       "      <th>context_cleaned</th>\n",
       "      <th>person_matched</th>\n",
       "      <th>company_matched</th>\n",
       "      <th>context_keywords</th>\n",
       "      <th>sign</th>\n",
       "      <th>input_text</th>\n",
       "      <th>tag_char</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>token_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023063021774831849&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>【北水手影】转净入16.6亿 买盈富中移动 沽汇控 腾讯。港股半年结日好淡争持，收市跌17点...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '盈富...</td>\n",
       "      <td>['中移动', '美团', '盈富', '腾讯', '兖矿能源', 'JS环球生活', '中...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['中移动', '美团', '小鹏', '腾讯', '盈富', '兖矿能源', '兖矿', ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>【北水手影】转净入16.6亿 买盈富中移动 沽汇控 腾讯。港股半年结日好淡争持，收市跌17点...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[中移动, 美团, 盈富, 腾讯, 兖矿能源, JS环球生活, 中芯, 理想汽车, 工行, ...</td>\n",
       "      <td>【北水手影】转净入16.6亿 买盈富中移动 沽汇控 腾讯。||可能的实体：中移动,盈富,腾讯...</td>\n",
       "      <td>False</td>\n",
       "      <td>【北水手影】转净入16.6亿 买盈富中移动 沽汇控 腾讯。港股半年结日好淡争持，收市跌17点...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[【, 北, 水, 手, 影, 】, 转, 净, 入, 1, 6, ., 6, 亿, 买, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023063021774832177&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>茂宸集团(00273) 延迟发送通函或其他文件 / 《收购守则》所指的受要约公司刊发的公告 ...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '茂宸...</td>\n",
       "      <td>['茂宸集团控股有限公司']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['茂宸集团', '茂宸集团控股有限公司']</td>\n",
       "      <td>[]</td>\n",
       "      <td>茂宸集团(00273) 延迟发送通函或其他文件 / 《收购守则》所指的受要约公司刊发的公告 ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[茂宸集团控股有限公司]</td>\n",
       "      <td>茂宸集团(00273) 延迟发送通函或其他文件 / 《收购守则》所指的受要约公司刊发的公告 ...</td>\n",
       "      <td>False</td>\n",
       "      <td>茂宸集团(00273) 延迟发送通函或其他文件 / 《收购守则》所指的受要约公司刊发的公告 ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[茂, 宸, 集, 团, (, 0, 0, 2, 7, 3, ), 延, 迟, 发, 送, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023063021774832208&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>日本“一口马”公开招股。日本“一口马”公开招股。每年七月都是日本马迷投资“一口马”，入股做马...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '賽馬...</td>\n",
       "      <td>['星岛头条', '赛马团体Silk Racing Co. Ltd', '文杰', '国枝荣']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>['星岛头条', '赛马团体Silk Racing Co. Ltd']</td>\n",
       "      <td>['文杰', '国枝荣']</td>\n",
       "      <td>日本“一口马”公开招股。日本“一口马”公开招股。每年七月都是日本马迷投资“一口马”，入股做马...</td>\n",
       "      <td>[文杰, 国枝荣]</td>\n",
       "      <td>[星岛头条, 赛马团体Silk Racing Co. Ltd]</td>\n",
       "      <td>日本“一口马”公开招股。日本“一口马”公开招股。每年七月都是日本马迷投资“一口马”，入股做马...</td>\n",
       "      <td>False</td>\n",
       "      <td>日本“一口马”公开招股。日本“一口马”公开招股。每年七月都是日本马迷投资“一口马”，入股做马...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[日, 本, [UNK], 一, 口, 马, [UNK], 公, 开, 招, 股, 。, 日...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    docid  headline  \\\n",
       "0  2023063021774831849&&0       NaN   \n",
       "1  2023063021774832177&&0       NaN   \n",
       "2  2023063021774832208&&0       NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  【北水手影】转净入16.6亿 买盈富中移动 沽汇控 腾讯。港股半年结日好淡争持，收市跌17点...   \n",
       "1  茂宸集团(00273) 延迟发送通函或其他文件 / 《收购守则》所指的受要约公司刊发的公告 ...   \n",
       "2  日本“一口马”公开招股。日本“一口马”公开招股。每年七月都是日本马迷投资“一口马”，入股做马...   \n",
       "\n",
       "                                                 ner  \\\n",
       "0  [{'label_name': 'Company', 'text_segment': '盈富...   \n",
       "1  [{'label_name': 'Company', 'text_segment': '茂宸...   \n",
       "2  [{'label_name': 'Company', 'text_segment': '賽馬...   \n",
       "\n",
       "                                    matched_keywords  include_company  \\\n",
       "0  ['中移动', '美团', '盈富', '腾讯', '兖矿能源', 'JS环球生活', '中...             True   \n",
       "1                                     ['茂宸集团控股有限公司']             True   \n",
       "2   ['星岛头条', '赛马团体Silk Racing Co. Ltd', '文杰', '国枝荣']             True   \n",
       "\n",
       "   include_person  split_sentence_index  \\\n",
       "0            True                     0   \n",
       "1            True                     0   \n",
       "2            True                     0   \n",
       "\n",
       "                                            Companys        Persons  \\\n",
       "0  ['中移动', '美团', '小鹏', '腾讯', '盈富', '兖矿能源', '兖矿', ...             []   \n",
       "1                             ['茂宸集团', '茂宸集团控股有限公司']             []   \n",
       "2                ['星岛头条', '赛马团体Silk Racing Co. Ltd']  ['文杰', '国枝荣']   \n",
       "\n",
       "                                     context_cleaned person_matched  \\\n",
       "0  【北水手影】转净入16.6亿 买盈富中移动 沽汇控 腾讯。港股半年结日好淡争持，收市跌17点...             []   \n",
       "1  茂宸集团(00273) 延迟发送通函或其他文件 / 《收购守则》所指的受要约公司刊发的公告 ...             []   \n",
       "2  日本“一口马”公开招股。日本“一口马”公开招股。每年七月都是日本马迷投资“一口马”，入股做马...      [文杰, 国枝荣]   \n",
       "\n",
       "                                     company_matched  \\\n",
       "0  [中移动, 美团, 盈富, 腾讯, 兖矿能源, JS环球生活, 中芯, 理想汽车, 工行, ...   \n",
       "1                                       [茂宸集团控股有限公司]   \n",
       "2                    [星岛头条, 赛马团体Silk Racing Co. Ltd]   \n",
       "\n",
       "                                    context_keywords   sign  \\\n",
       "0  【北水手影】转净入16.6亿 买盈富中移动 沽汇控 腾讯。||可能的实体：中移动,盈富,腾讯...  False   \n",
       "1  茂宸集团(00273) 延迟发送通函或其他文件 / 《收购守则》所指的受要约公司刊发的公告 ...  False   \n",
       "2  日本“一口马”公开招股。日本“一口马”公开招股。每年七月都是日本马迷投资“一口马”，入股做马...  False   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  【北水手影】转净入16.6亿 买盈富中移动 沽汇控 腾讯。港股半年结日好淡争持，收市跌17点...   \n",
       "1  茂宸集团(00273) 延迟发送通函或其他文件 / 《收购守则》所指的受要约公司刊发的公告 ...   \n",
       "2  日本“一口马”公开招股。日本“一口马”公开招股。每年七月都是日本马迷投资“一口马”，入股做马...   \n",
       "\n",
       "                                            tag_char  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                   tokenized_content  \\\n",
       "0  [【, 北, 水, 手, 影, 】, 转, 净, 入, 1, 6, ., 6, 亿, 买, ...   \n",
       "1  [茂, 宸, 集, 团, (, 0, 0, 2, 7, 3, ), 延, 迟, 发, 送, ...   \n",
       "2  [日, 本, [UNK], 一, 口, 马, [UNK], 公, 开, 招, 股, 。, 日...   \n",
       "\n",
       "                                        token_labels  \n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "1  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...  "
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df_valid.shape)\n",
    "df_valid = df_valid[~df_valid['tag_char'].isna()]\n",
    "print(df_valid.shape)\n",
    "df_valid = tokenize_and_align_labels(df_valid, args)\n",
    "df_valid.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# remove long text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.max_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    13382.000000\n",
      "mean       358.544911\n",
      "std        163.292806\n",
      "min          4.000000\n",
      "25%        215.000000\n",
      "50%        441.000000\n",
      "75%        501.000000\n",
      "max        509.000000\n",
      "Name: len_tokens, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_train['len_tokens'] = df_train['tokenized_content'].apply(lambda x: len(x))\n",
    "df_train = df_train[df_train['len_tokens'] < args.max_len-2]\n",
    "df_train = df_train.reset_index(drop=True)\n",
    "print(df_train['len_tokens'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    1674.000000\n",
      "mean      349.482676\n",
      "std       168.519374\n",
      "min         6.000000\n",
      "25%       191.250000\n",
      "50%       425.500000\n",
      "75%       501.000000\n",
      "max       509.000000\n",
      "Name: len_tokens, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "df_valid['len_tokens'] = df_valid['tokenized_content'].apply(lambda x: len(x))\n",
    "df_valid = df_valid[df_valid['len_tokens'] < args.max_len-2]\n",
    "df_valid = df_valid.reset_index(drop=True)\n",
    "print(df_valid['len_tokens'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('近', 'O'), ('期', 'O'), ('收', 'O'), ('益', 'O'), ('率', 'O'), ('排', 'O'), ('名', 'O'), ('靠', 'O'), ('前', 'O'), ('，', 'O'), ('依', 'O'), ('据', 'O'), ('货', 'O'), ('币', 'O'), ('型', 'O'), ('基', 'O'), ('金', 'O'), ('[UNK]', 'O'), ('买', 'O'), ('涨', 'O'), ('不', 'O'), ('买', 'O'), ('跌', 'O'), ('[UNK]', 'O'), ('的', 'O'), ('原', 'O'), ('则', 'O'), ('，', 'O'), ('短', 'O'), ('期', 'O'), ('可', 'O'), ('关', 'O'), ('注', 'O'), ('。', 'O'), ('低', 'O'), ('风', 'O'), ('险', 'O'), ('。', 'O'), ('立', 'O'), ('即', 'O'), ('购', 'O'), ('买', 'O'), ('。', 'O'), ('2', 'O'), ('、', 'O'), ('新', 'O'), ('发', 'O'), ('基', 'O'), ('金', 'O'), ('专', 'O'), ('区', 'O'), ('。', 'O'), ('基', 'O'), ('金', 'O'), ('代', 'O'), ('码', 'O'), ('。', 'O'), ('基', 'O'), ('金', 'O'), ('名', 'O'), ('称', 'O'), ('。', 'O'), ('开', 'O'), ('放', 'O'), ('日', 'O'), ('期', 'O'), ('。', 'O'), ('截', 'O'), ('止', 'O'), ('日', 'O'), ('期', 'O'), ('。', 'O'), ('操', 'O'), ('作', 'O'), ('。', 'O'), ('0', 'O'), ('0', 'O'), ('2', 'O'), ('7', 'O'), ('0', 'O'), ('8', 'O'), ('。', 'O'), ('大', 'O'), ('摩', 'O'), ('健', 'O'), ('康', 'O'), ('产', 'O'), ('业', 'O'), ('混', 'O'), ('合', 'O'), ('。', 'O'), ('2', 'O'), ('0', 'O'), ('1', 'O'), ('6', 'O'), ('.', 'O'), ('5', 'O'), ('.', 'O'), ('2', 'O'), ('6', 'O'), ('。', 'O'), ('2', 'O'), ('0', 'O'), ('1', 'O'), ('6', 'O'), ('.', 'O'), ('6', 'O'), ('.', 'O'), ('2', 'O'), ('4', 'O'), ('。', 'O'), ('立', 'O'), ('即', 'O'), ('购', 'O'), ('买', 'O'), ('。', 'O'), ('三', 'O'), ('、', 'O'), ('本', 'O'), ('周', 'O'), ('重', 'O'), ('点', 'O'), ('新', 'O'), ('闻', 'O'), ('回', 'O'), ('顾', 'O'), ('。', 'O'), ('1', 'O'), ('、', 'O'), ('第', 'O'), ('二', 'O'), ('批', 'O'), ('国', 'O'), ('企', 'O'), ('改', 'O'), ('革', 'O'), ('试', 'O'), ('点', 'O'), ('名', 'O'), ('单', 'O'), ('将', 'O'), ('公', 'O'), ('布', 'O'), ('。', 'O'), ('2', 'O'), ('日', 'O'), ('，', 'O'), ('国', 'B-Company'), ('务', 'I-Company'), ('院', 'I-Company'), ('国', 'O'), ('资', 'O'), ('委', 'O'), ('主', 'O'), ('任', 'O'), ('肖', 'B-Person'), ('亚', 'I-Person'), ('庆', 'I-Person'), ('在', 'O'), ('中', 'B-Company'), ('国', 'I-Company'), ('五', 'I-Company'), ('矿', 'I-Company'), ('与', 'O'), ('中', 'B-Company'), ('冶', 'I-Company'), ('集', 'I-Company'), ('团', 'I-Company'), ('重', 'O'), ('组', 'O'), ('大', 'O'), ('会', 'O'), ('上', 'O'), ('透', 'O'), ('露', 'O'), ('，', 'O'), ('新', 'O'), ('组', 'O'), ('建', 'O'), ('的', 'O'), ('中', 'B-Company'), ('国', 'I-Company'), ('五', 'I-Company'), ('矿', 'I-Company'), ('已', 'O'), ('纳', 'O'), ('入', 'O'), ('国', 'O'), ('有', 'O'), ('企', 'O'), ('业', 'O'), ('改', 'O'), ('革', 'O'), ('试', 'O'), ('点', 'O'), ('，', 'O'), ('他', 'O'), ('勉', 'O'), ('励', 'O'), ('中', 'B-Company'), ('国', 'I-Company'), ('五', 'I-Company'), ('矿', 'I-Company'), ('要', 'O'), ('通', 'O'), ('过', 'O'), ('改', 'O'), ('革', 'O'), ('试', 'O'), ('点', 'O'), ('出', 'O'), ('成', 'O'), ('效', 'O'), ('。', 'O'), ('据', 'O'), ('知', 'O'), ('情', 'O'), ('人', 'O'), ('士', 'O'), ('称', 'O'), ('，', 'O'), ('中', 'B-Company'), ('国', 'I-Company'), ('五', 'I-Company'), ('矿', 'I-Company'), ('将', 'O'), ('开', 'O'), ('展', 'O'), ('国', 'O'), ('有', 'O'), ('资', 'O'), ('本', 'O'), ('投', 'O'), ('资', 'O'), ('公', 'O'), ('司', 'O'), ('试', 'O'), ('点', 'O'), ('。', 'O'), ('今', 'O'), ('年', 'O'), ('2', 'O'), ('月', 'O'), ('，', 'O'), ('国', 'B-Company'), ('务', 'I-Company'), ('院', 'I-Company'), ('国', 'O'), ('资', 'O'), ('委', 'O'), ('副', 'O'), ('主', 'O'), ('任', 'O'), ('张', 'B-Person'), ('喜', 'I-Person'), ('武', 'I-Person'), ('透', 'O'), ('露', 'O'), ('诚', 'B-Company'), ('通', 'I-Company'), ('集', 'I-Company'), ('团', 'I-Company'), ('和', 'O'), ('国', 'B-Company'), ('新', 'I-Company'), ('公', 'I-Company'), ('司', 'I-Company'), ('两', 'O'), ('家', 'O'), ('中', 'O'), ('央', 'O'), ('企', 'O'), ('业', 'O'), ('确', 'O'), ('定', 'O'), ('为', 'O'), ('国', 'O'), ('有', 'O'), ('资', 'O'), ('本', 'O'), ('运', 'O'), ('营', 'O'), ('公', 'O'), ('司', 'O'), ('试', 'O'), ('点', 'O'), ('企', 'O'), ('业', 'O'), ('。', 'O'), ('市', 'O'), ('场', 'O'), ('普', 'O'), ('遍', 'O'), ('预', 'O'), ('计', 'O'), ('，', 'O'), ('第', 'O'), ('二', 'O'), ('批', 'O'), ('央', 'O'), ('企', 'O'), ('改', 'O'), ('革', 'O'), ('试', 'O'), ('点', 'O'), ('名', 'O'), ('单', 'O'), ('将', 'O'), ('于', 'O'), ('今', 'O'), ('年', 'O'), ('上', 'O'), ('半', 'O'), ('年', 'O'), ('推', 'O'), ('出', 'O'), ('，', 'O'), ('公', 'O'), ('布', 'O'), ('时', 'O'), ('间', 'O'), ('窗', 'O'), ('口', 'O'), ('正', 'O'), ('在', 'O'), ('临', 'O'), ('近', 'O'), ('。', 'O'), ('相', 'O'), ('较', 'O'), ('于', 'O'), ('第', 'O'), ('一', 'O'), ('批', 'O'), ('试', 'O'), ('点', 'O'), ('名', 'O'), ('单', 'O'), ('基', 'O'), ('于', 'O'), ('[UNK]', 'O'), ('四', 'O'), ('项', 'O'), ('试', 'O'), ('点', 'O'), ('[UNK]', 'O'), ('产', 'O'), ('生', 'O'), ('，', 'O'), ('第', 'O'), ('二', 'O'), ('批', 'O'), ('是', 'O'), ('在', 'O'), ('[UNK]', 'O'), ('十', 'O'), ('项', 'O'), ('试', 'O'), ('点', 'O'), ('[UNK]', 'O'), ('基', 'O'), ('础', 'O'), ('上', 'O'), ('推', 'O'), ('进', 'O'), ('，', 'O'), ('结', 'O'), ('合', 'O'), ('今', 'O'), ('年', 'O'), ('高', 'O'), ('层', 'O'), ('大', 'O'), ('力', 'O'), ('推', 'O'), ('进', 'O'), ('供', 'O'), ('给', 'O'), ('侧', 'O'), ('改', 'O'), ('革', 'O'), ('，', 'O'), ('第', 'O'), ('二', 'O'), ('批', 'O'), ('改', 'O'), ('革', 'O'), ('试', 'O'), ('点', 'O'), ('的', 'O'), ('力', 'O'), ('度', 'O'), ('、', 'O'), ('深', 'O'), ('度', 'O'), ('上', 'O'), ('或', 'O'), ('将', 'O'), ('更', 'O'), ('大', 'O'), ('。', 'O'), ('相', 'O'), ('关', 'O'), ('品', 'O'), ('种', 'O'), ('：', 'O'), ('2', 'O'), ('4', 'O'), ('0', 'O'), ('0', 'O'), ('2', 'O'), ('2', 'O'), ('华', 'O'), ('宝', 'O'), ('资', 'O'), ('源', 'O'), ('优', 'O'), ('选', 'O'), ('混', 'O'), ('合', 'O'), ('（', 'O'), ('查', 'O'), ('看', 'O'), ('详', 'O'), ('情', 'O'), ('，', 'O'), ('立', 'O'), ('即', 'O'), ('购', 'O'), ('买', 'O'), ('）', 'O'), ('。', 'O'), ('2', 'O'), ('、', 'O'), ('量', 'O'), ('子', 'O'), ('卫', 'O'), ('星', 'O'), ('或', 'O'), ('于', 'O'), ('7', 'O'), ('月', 'O'), ('发', 'O'), ('射', 'O'), ('升', 'O'), ('空', 'O'), ('。', 'O'), ('据', 'O'), ('公', 'O'), ('开', 'O'), ('资', 'O'), ('料', 'O'), ('显', 'O'), ('示', 'O'), ('，', 'O'), ('量', 'O'), ('子', 'O'), ('通', 'O'), ('信', 'O'), ('在', 'O'), ('军', 'O'), ('事', 'O'), ('、', 'O'), ('国', 'O'), ('防', 'O'), ('、', 'O'), ('金', 'O'), ('融', 'O'), ('等', 'O'), ('信', 'O'), ('息', 'O'), ('安', 'O'), ('全', 'O'), ('领', 'O'), ('域', 'O'), ('有', 'O'), ('着', 'O'), ('重', 'O'), ('大', 'O'), ('的', 'O'), ('应', 'O'), ('用', 'O'), ('价', 'O'), ('值', 'O'), ('和', 'O'), ('前', 'O'), ('景', 'O'), ('，', 'O'), ('未', 'O'), ('来', 'O'), ('国', 'O'), ('内', 'O'), ('量', 'O'), ('子', 'O'), ('通', 'O'), ('信', 'O'), ('市', 'O'), ('场', 'O'), ('规', 'O'), ('模', 'O'), ('有', 'O'), ('望', 'O'), ('达', 'O'), ('到', 'O'), ('千', 'O'), ('亿', 'O'), ('元', 'O'), ('级', 'O'), ('别', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(13382, 20)"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(zip(df_train.loc[10300, 'tokenized_content'], df_train.loc[10300, 'token_labels'])))\n",
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('内', 'O'), ('容', 'O'), ('推', 'O'), ('荐', 'O'), ('。', 'O'), ('死', 'O'), ('得', 'O'), ('漂', 'O'), ('亮', 'O'), ('的', 'O'), ('方', 'O'), ('法', 'O'), ('是', 'O'), ('什', 'O'), ('么', 'O'), ('。', 'O'), ('总', 'O'), ('之', 'O'), ('纳', 'B-Person'), ('斯', 'I-Person'), ('丽', 'I-Person'), ('安', 'O'), ('静', 'O'), ('地', 'O'), ('死', 'O'), ('去', 'O'), ('了', 'O'), ('。', 'O'), ('本', 'O'), ('来', 'O'), ('，', 'O'), ('我', 'O'), ('想', 'O'), ('让', 'O'), ('她', 'O'), ('的', 'O'), ('尸', 'O'), ('体', 'O'), ('笔', 'O'), ('直', 'O'), ('伸', 'O'), ('展', 'O'), ('，', 'O'), ('躺', 'O'), ('在', 'O'), ('里', 'O'), ('面', 'O'), ('，', 'O'), ('不', 'O'), ('料', 'O'), ('冰', 'O'), ('柜', 'O'), ('的', 'O'), ('尺', 'O'), ('寸', 'O'), ('有', 'O'), ('点', 'O'), ('小', 'O'), ('，', 'O'), ('不', 'O'), ('弯', 'O'), ('起', 'O'), ('她', 'O'), ('的', 'O'), ('膝', 'O'), ('盖', 'O'), ('就', 'O'), ('收', 'O'), ('不', 'O'), ('进', 'O'), ('去', 'O'), ('。', 'O'), ('那', 'O'), ('模', 'O'), ('样', 'O'), ('说', 'O'), ('是', 'O'), ('吸', 'O'), ('血', 'O'), ('鬼', 'O'), ('的', 'O'), ('新', 'O'), ('娘', 'O'), ('，', 'O'), ('倒', 'O'), ('不', 'O'), ('如', 'O'), ('说', 'O'), ('是', 'O'), ('印', 'O'), ('加', 'O'), ('帝', 'O'), ('国', 'O'), ('的', 'O'), ('木', 'O'), ('乃', 'O'), ('伊', 'O'), ('。', 'O'), ('我', 'O'), ('享', 'O'), ('受', 'O'), ('着', 'O'), ('纳', 'B-Person'), ('斯', 'I-Person'), ('丽', 'I-Person'), ('的', 'O'), ('鲜', 'O'), ('血', 'O'), ('，', 'O'), ('然', 'O'), ('后', 'O'), ('合', 'O'), ('上', 'O'), ('冰', 'O'), ('柜', 'O'), ('的', 'O'), ('门', 'O'), ('。', 'O'), ('纳', 'B-Person'), ('斯', 'I-Person'), ('丽', 'I-Person'), ('沉', 'O'), ('入', 'O'), ('永', 'O'), ('远', 'O'), ('的', 'O'), ('睡', 'O'), ('眠', 'O')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1674, 20)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(list(zip(df_valid.loc[1310, 'tokenized_content'], df_valid.loc[1310, 'token_labels'])))\n",
    "df_valid.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 统计有公司的文本量和有人名的文本量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12329, 20) (11109, 20) Index(['docid', 'headline', 'content', 'ner', 'matched_keywords',\n",
      "       'include_company', 'include_person', 'split_sentence_index', 'Companys',\n",
      "       'Persons', 'context_cleaned', 'person_matched', 'company_matched',\n",
      "       'context_keywords', 'sign', 'input_text', 'tag_char',\n",
      "       'tokenized_content', 'token_labels', 'len_tokens'],\n",
      "      dtype='object')\n",
      "(1533, 20) (1391, 20) Index(['docid', 'headline', 'content', 'ner', 'matched_keywords',\n",
      "       'include_company', 'include_person', 'split_sentence_index', 'Companys',\n",
      "       'Persons', 'context_cleaned', 'person_matched', 'company_matched',\n",
      "       'context_keywords', 'sign', 'input_text', 'tag_char',\n",
      "       'tokenized_content', 'token_labels', 'len_tokens'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_train[df_train['include_company']==True].shape, df_train[df_train['include_person']==True].shape, df_train.columns)\n",
    "print(df_valid[df_valid['include_company']==True].shape, df_valid[df_valid['include_person']==True].shape, df_valid.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((13382, 20), (1674, 20))"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape, df_valid.shape, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../experiments/0530_bert_p=0.5_pos=sentence_BP'"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.prefix_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_train.to_pickle(f\"{args.prefix_path}/train_data.pkl\")\n",
    "df_valid.to_pickle(f\"{args.prefix_path}/valid_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_pickle(f\"{args.prefix_path}/train_data.pkl\")\n",
    "# df_valid = pd.read_pickle(f\"{args.prefix_path}/valid_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for idx, row in df_train.iterrows():\n",
    "    if \"peets\" in row['input_text'].lower():\n",
    "        print(row['input_text'])\n",
    "        for i, j in zip(row['tokenized_content'], row['token_labels']):\n",
    "            print(i, j)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_padding_and_mask(example):\n",
    "    tokenized_context = [tokenizer.cls_token] + example['tokenized_content'] + [tokenizer.sep_token]\n",
    "    labels = example['token_labels']\n",
    "    labels.insert(0, 'O')\n",
    "    labels.insert(len(labels), \"O\")\n",
    "\n",
    "    max_len = args.max_len\n",
    "    if len(tokenized_context) > max_len: \n",
    "        tokenized_context = tokenized_context[:max_len]\n",
    "        labels = labels[:max_len]\n",
    "    else:\n",
    "        tokenized_context = tokenized_context + [tokenizer.pad_token] * (max_len - len(tokenized_context))\n",
    "        labels = labels + ['O'] * (max_len - len(labels))\n",
    "\n",
    "    attn_mask = [1 if tok != tokenizer.pad_token else 0 for tok in tokenized_context]\n",
    "\n",
    "    ids = tokenizer.convert_tokens_to_ids(tokenized_context)\n",
    "\n",
    "    label_ids = [labels_to_ids[label] for label in labels]\n",
    "\n",
    "    return {\n",
    "          'ids': torch.tensor(ids, dtype=torch.long),\n",
    "          'masks': torch.tensor(attn_mask, dtype=torch.long),\n",
    "          'labels': torch.tensor(label_ids, dtype=torch.long)\n",
    "        } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfca89ca1f0d4e31920d90b75d33c83a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Map', max=13382.0, style=ProgressStyle(description_width=…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "train_dataset = datasets.Dataset.from_pandas(df_train[['docid', 'tokenized_content', 'token_labels']])\n",
    "train_dataset = train_dataset.map(add_padding_and_mask, remove_columns=['tokenized_content', 'token_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97653ff156d146ceb6d8974e6e120045",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Map', max=1674.0, style=ProgressStyle(description_width='…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(args.pretrained_model)\n",
    "\n",
    "valid_dataset = datasets.Dataset.from_pandas(df_valid[['docid', 'tokenized_content', 'token_labels']])\n",
    "valid_dataset = valid_dataset.map(add_padding_and_mask, remove_columns=['tokenized_content', 'token_labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c54b7dee056b4b2a926de37b2cd43495",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Saving the dataset (0/1 shards)', max=13382.0, style=Prog…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1482a98d27749678959ae481e059dfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Saving the dataset (0/1 shards)', max=1674.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "train_dataset.save_to_disk(f'{args.prefix_path}/train_dataset')\n",
    "valid_dataset.save_to_disk(f'{args.prefix_path}/valid_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets.Dataset.load_from_disk(f'{args.pre_path}/dataset/headline_content_keywords/train/train/train_data')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3710, 16)\n",
      "INFO: Pandarallel will run on 32 workers.\n",
      "INFO: Pandarallel will use standard multiprocessing data transfer (pipe) to transfer data between the main process and workers.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "751db7af65814372bd03d82eb6b8fef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=116), Label(value='0 / 116'))), HB…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.8/site-packages/numpy/core/fromnumeric.py:3156: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  return asarray(a).ndim\n"
     ]
    }
   ],
   "source": [
    "df_test[\"input_text\"] = df_test[\"context_cleaned\"] ## 全不加\n",
    "# df_test[\"input_text\"] = df_test[\"context_keywords\"] ## 全加keywords\n",
    "print(df_test.shape)\n",
    "pandarallel.initialize(nb_workers=32, progress_bar=True, use_memory_fs=False)\n",
    "df_test[[\"tokenized_content\", \"ids\", \"masks\"]] = df_test.parallel_apply(tokenize_test_text, param_args=args, axis=1).to_list()\n",
    "\n",
    "test_dataset = datasets.Dataset.from_pandas(df_test[[\"docid\", \"tokenized_content\", \"ids\", \"masks\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3710, 19)\n"
     ]
    }
   ],
   "source": [
    "# df_test.to_pickle(f\"{args.pre_path}/dataset/headline_content_keywords/test/test_data_{args.experiment_name}_{args.mode}.pkl\")\n",
    "print(df_test.shape)\n",
    "df_test.to_pickle(f\"{args.prefix_path}/test_data.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a6cff9c4b914f3fac3847fb176a1f49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Saving the dataset (0/1 shards)', max=3710.0, style=Progr…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# test_dataset.save_to_disk(f'{args.pre_path}/dataset/headline_content_keywords/test/test/test_data_new-dict')\n",
    "test_dataset.save_to_disk(f'{args.prefix_path}/test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25793/977533159.py:1: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  df_train.head(1).to_dict('record')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'docid': '2023063021774796465&&0',\n",
       "  'headline': nan,\n",
       "  'content': '宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌状况及继续暂停买卖之季度情况更新(558KB, pdf)',\n",
       "  'ner': \"[{'label_name': 'Company', 'text_segment': '寶申控股', 'start_ind': 0, 'end_ind': 4}, {'label_name': 'Company', 'text_segment': '寶申控股', 'start_ind': 31, 'end_ind': 35}]\",\n",
       "  'matched_keywords': \"['宝申控股']\",\n",
       "  'include_company': True,\n",
       "  'include_person': True,\n",
       "  'split_sentence_index': 0,\n",
       "  'Companys': \"['宝申控股']\",\n",
       "  'Persons': '[]',\n",
       "  'context_cleaned': '宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌状况及继续暂停买卖之季度情况更新(558kb, pdf)',\n",
       "  'person_matched': [],\n",
       "  'company_matched': ['宝申控股'],\n",
       "  'context_keywords': '宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。||可能的实体：宝申控股||宝申控股(08151) 有关复牌状况及继续暂停买卖之季度情况更新(558kb, pdf)||可能的实体：宝申控股||',\n",
       "  'sign': False,\n",
       "  'input_text': '宝申控股(08151) 停牌 / 内幕消息 / 其他-杂项。宝申控股(08151) 有关复牌状况及继续暂停买卖之季度情况更新(558kb, pdf)',\n",
       "  'tag_char': ['B-Company',\n",
       "   'I-Company',\n",
       "   'I-Company',\n",
       "   'I-Company',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Company',\n",
       "   'I-Company',\n",
       "   'I-Company',\n",
       "   'I-Company',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  'tokenized_content': ['宝',\n",
       "   '申',\n",
       "   '控',\n",
       "   '股',\n",
       "   '(',\n",
       "   '0',\n",
       "   '8',\n",
       "   '1',\n",
       "   '5',\n",
       "   '1',\n",
       "   ')',\n",
       "   '停',\n",
       "   '牌',\n",
       "   '/',\n",
       "   '内',\n",
       "   '幕',\n",
       "   '消',\n",
       "   '息',\n",
       "   '/',\n",
       "   '其',\n",
       "   '他',\n",
       "   '-',\n",
       "   '杂',\n",
       "   '项',\n",
       "   '。',\n",
       "   '宝',\n",
       "   '申',\n",
       "   '控',\n",
       "   '股',\n",
       "   '(',\n",
       "   '0',\n",
       "   '8',\n",
       "   '1',\n",
       "   '5',\n",
       "   '1',\n",
       "   ')',\n",
       "   '有',\n",
       "   '关',\n",
       "   '复',\n",
       "   '牌',\n",
       "   '状',\n",
       "   '况',\n",
       "   '及',\n",
       "   '继',\n",
       "   '续',\n",
       "   '暂',\n",
       "   '停',\n",
       "   '买',\n",
       "   '卖',\n",
       "   '之',\n",
       "   '季',\n",
       "   '度',\n",
       "   '情',\n",
       "   '况',\n",
       "   '更',\n",
       "   '新',\n",
       "   '(',\n",
       "   '5',\n",
       "   '5',\n",
       "   '8',\n",
       "   'k',\n",
       "   'b',\n",
       "   ',',\n",
       "   'p',\n",
       "   'd',\n",
       "   'f',\n",
       "   ')'],\n",
       "  'token_labels': ['B-Company',\n",
       "   'I-Company',\n",
       "   'I-Company',\n",
       "   'I-Company',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'B-Company',\n",
       "   'I-Company',\n",
       "   'I-Company',\n",
       "   'I-Company',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O',\n",
       "   'O'],\n",
       "  'len_tokens': 67}]"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head(1).to_dict('record')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

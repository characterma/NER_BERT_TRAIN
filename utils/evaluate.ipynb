{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import yaml\n",
    "import pandas as pd\n",
    "\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(batch_size=32, date='0606', do_prediction=True, do_training=True, entity_type=['Company', 'Person'], experiment_name='bert_p=0.5_pos=sentence', lr='1e-5', max_len=512, mode='BP', n_epochs=5, num_labels=5, prefix_path='../experiments/0606_bert_p=0.5_pos=sentence_BP', pretrained_model='hfl/chinese-roberta-wwm-ext', test_data='/test_dataset', threshold=0.5, train_data='/train_dataset', valid_data='/valid_dataset')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"../config.yaml\", 'r') as config_file:\n",
    "    config = yaml.safe_load(config_file)\n",
    "\n",
    "parser = argparse.ArgumentParser(description='') \n",
    "parser.set_defaults(**config)\n",
    "args = parser.parse_args(args = [])\n",
    "\n",
    "parser.add_argument(\"--prefix_path\", type=str, default=f\"../experiments/{args.date}_{args.experiment_name}_{args.mode}\")\n",
    "args = parser.parse_args(args = [])\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_test_path = args.prefix_path + \"/test_data.pkl\"\n",
    "df_output_path = args.prefix_path + \"/test_data_predict_result.pkl\"\n",
    "df_save_path = args.prefix_path + \"/test_data_predict_result_all.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_test_path = args.prefix_path + \"/badcase_df.pkl\"\n",
    "# df_output_path = args.prefix_path + \"/test_data_predict_result.pkl\"\n",
    "# df_save_path = args.prefix_path + \"/badcase_data_predict_result_all.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4554, 20)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_pickle(df_test_path)\n",
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>ner</th>\n",
       "      <th>matched_keywords</th>\n",
       "      <th>include_company</th>\n",
       "      <th>include_person</th>\n",
       "      <th>length</th>\n",
       "      <th>split_sentence_index</th>\n",
       "      <th>Companys</th>\n",
       "      <th>Persons</th>\n",
       "      <th>context_cleaned</th>\n",
       "      <th>person_matched</th>\n",
       "      <th>company_matched</th>\n",
       "      <th>context_keywords</th>\n",
       "      <th>sign</th>\n",
       "      <th>input_text</th>\n",
       "      <th>tokenized_content</th>\n",
       "      <th>ids</th>\n",
       "      <th>masks</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023063021774796605&amp;&amp;2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '21...</td>\n",
       "      <td>['黄如']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>471</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>['黄如']</td>\n",
       "      <td>突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...</td>\n",
       "      <td>[黄如]</td>\n",
       "      <td>[]</td>\n",
       "      <td>突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...</td>\n",
       "      <td>False</td>\n",
       "      <td>突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...</td>\n",
       "      <td>[突, 出, 科, 产, 融, 合, ,, 推, 动, 创, 新, 链, 、, 产, 业, ...</td>\n",
       "      <td>[101, 4960, 1139, 4906, 772, 6084, 1394, 117, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023063021774797155&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>曾国卫受邀回母校　鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校...</td>\n",
       "      <td>[{'label_name': 'Person', 'text_segment': '曾國衞...</td>\n",
       "      <td>['长和', '曾国卫', '赵霁']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>['长和']</td>\n",
       "      <td>['曾国卫', '赵霁']</td>\n",
       "      <td>曾国卫受邀回母校鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校东...</td>\n",
       "      <td>[曾国卫, 赵霁]</td>\n",
       "      <td>[长和]</td>\n",
       "      <td>曾国卫受邀回母校鼓励同学深入认识祖国。||可能的实体：曾国卫||政制及内地事务局局长曾国卫今...</td>\n",
       "      <td>False</td>\n",
       "      <td>曾国卫受邀回母校鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校东...</td>\n",
       "      <td>[曾, 国, 卫, 受, 邀, 回, 母, 校, 鼓, 励, 同, 学, 深, 入, 认, ...</td>\n",
       "      <td>[101, 3295, 1744, 1310, 1358, 6913, 1726, 3678...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023063021774797862&amp;&amp;1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由A轮的每股0.1...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '黑芝...</td>\n",
       "      <td>['汽车', '小米', 'SoC', '中国银行', '腾讯', '黑芝麻集资', 'Oc...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>482</td>\n",
       "      <td>1</td>\n",
       "      <td>['小米', '黑芝', 'Oceanpine', '北极光创投', '汽车', '黑芝麻'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由a轮的每股0.1...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[汽车, 小米, SoC, 中国银行, 腾讯, 黑芝麻集资, Oceanpine, 吉利集团...</td>\n",
       "      <td>初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由a轮的每股0.1...</td>\n",
       "      <td>False</td>\n",
       "      <td>初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由a轮的每股0.1...</td>\n",
       "      <td>[初, 步, 上, 市, 申, 请, 文, 件, 披, 露, ，, 黑, 芝, 麻, 2, ...</td>\n",
       "      <td>[101, 1159, 3635, 677, 2356, 4509, 6435, 3152,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023063021774798050&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '新明...</td>\n",
       "      <td>['新明中国']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>['新明中国']</td>\n",
       "      <td>[]</td>\n",
       "      <td>新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[新明中国]</td>\n",
       "      <td>新明中国(02699) 内幕消息 / 停牌。||可能的实体：新明中国||新明中国(02699...</td>\n",
       "      <td>False</td>\n",
       "      <td>新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...</td>\n",
       "      <td>[新, 明, 中, 国, (, 0, 2, 6, 9, 9, ), 内, 幕, 消, 息, ...</td>\n",
       "      <td>[101, 3173, 3209, 704, 1744, 113, 121, 123, 12...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023063021774805645&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '地政...</td>\n",
       "      <td>['新地', '长实', '会德丰地产', '信置', '嘉华国际', '交通', '招商局...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>293</td>\n",
       "      <td>0</td>\n",
       "      <td>['新地', '长实', '会德丰地产', '信置', '嘉华国际', '交通', '招商局...</td>\n",
       "      <td>[]</td>\n",
       "      <td>域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[新地, 长实, 会德丰地产, 信置, 嘉华国际, 交通, 招商局置地, 城巴, 鹰君, 东华]</td>\n",
       "      <td>域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...</td>\n",
       "      <td>False</td>\n",
       "      <td>域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...</td>\n",
       "      <td>[域, 多, 利, 道, 地, 皮, 接, 6, 标, 书, 。, 地, 政, 总, 署, ...</td>\n",
       "      <td>[101, 1818, 1914, 1164, 6887, 1765, 4649, 2970...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    docid  headline  \\\n",
       "0  2023063021774796605&&2       NaN   \n",
       "1  2023063021774797155&&0       NaN   \n",
       "2  2023063021774797862&&1       NaN   \n",
       "3  2023063021774798050&&0       NaN   \n",
       "4  2023063021774805645&&0       NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...   \n",
       "1  曾国卫受邀回母校　鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校...   \n",
       "2  初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由A轮的每股0.1...   \n",
       "3  新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...   \n",
       "4  域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...   \n",
       "\n",
       "                                                 ner  \\\n",
       "0  [{'label_name': 'Company', 'text_segment': '21...   \n",
       "1  [{'label_name': 'Person', 'text_segment': '曾國衞...   \n",
       "2  [{'label_name': 'Company', 'text_segment': '黑芝...   \n",
       "3  [{'label_name': 'Company', 'text_segment': '新明...   \n",
       "4  [{'label_name': 'Company', 'text_segment': '地政...   \n",
       "\n",
       "                                    matched_keywords  include_company  \\\n",
       "0                                             ['黄如']             True   \n",
       "1                                ['长和', '曾国卫', '赵霁']             True   \n",
       "2  ['汽车', '小米', 'SoC', '中国银行', '腾讯', '黑芝麻集资', 'Oc...             True   \n",
       "3                                           ['新明中国']             True   \n",
       "4  ['新地', '长实', '会德丰地产', '信置', '嘉华国际', '交通', '招商局...             True   \n",
       "\n",
       "   include_person  length  split_sentence_index  \\\n",
       "0            True     471                     2   \n",
       "1            True     496                     0   \n",
       "2            True     482                     1   \n",
       "3            True      55                     0   \n",
       "4            True     293                     0   \n",
       "\n",
       "                                            Companys        Persons  \\\n",
       "0                                                 []         ['黄如']   \n",
       "1                                             ['长和']  ['曾国卫', '赵霁']   \n",
       "2  ['小米', '黑芝', 'Oceanpine', '北极光创投', '汽车', '黑芝麻'...             []   \n",
       "3                                           ['新明中国']             []   \n",
       "4  ['新地', '长实', '会德丰地产', '信置', '嘉华国际', '交通', '招商局...             []   \n",
       "\n",
       "                                     context_cleaned person_matched  \\\n",
       "0  突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...           [黄如]   \n",
       "1  曾国卫受邀回母校鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校东...      [曾国卫, 赵霁]   \n",
       "2  初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由a轮的每股0.1...             []   \n",
       "3  新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...             []   \n",
       "4  域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...             []   \n",
       "\n",
       "                                     company_matched  \\\n",
       "0                                                 []   \n",
       "1                                               [长和]   \n",
       "2  [汽车, 小米, SoC, 中国银行, 腾讯, 黑芝麻集资, Oceanpine, 吉利集团...   \n",
       "3                                             [新明中国]   \n",
       "4   [新地, 长实, 会德丰地产, 信置, 嘉华国际, 交通, 招商局置地, 城巴, 鹰君, 东华]   \n",
       "\n",
       "                                    context_keywords   sign  \\\n",
       "0  突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...  False   \n",
       "1  曾国卫受邀回母校鼓励同学深入认识祖国。||可能的实体：曾国卫||政制及内地事务局局长曾国卫今...  False   \n",
       "2  初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由a轮的每股0.1...  False   \n",
       "3  新明中国(02699) 内幕消息 / 停牌。||可能的实体：新明中国||新明中国(02699...  False   \n",
       "4  域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...  False   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...   \n",
       "1  曾国卫受邀回母校鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校东...   \n",
       "2  初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由a轮的每股0.1...   \n",
       "3  新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...   \n",
       "4  域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...   \n",
       "\n",
       "                                   tokenized_content  \\\n",
       "0  [突, 出, 科, 产, 融, 合, ,, 推, 动, 创, 新, 链, 、, 产, 业, ...   \n",
       "1  [曾, 国, 卫, 受, 邀, 回, 母, 校, 鼓, 励, 同, 学, 深, 入, 认, ...   \n",
       "2  [初, 步, 上, 市, 申, 请, 文, 件, 披, 露, ，, 黑, 芝, 麻, 2, ...   \n",
       "3  [新, 明, 中, 国, (, 0, 2, 6, 9, 9, ), 内, 幕, 消, 息, ...   \n",
       "4  [域, 多, 利, 道, 地, 皮, 接, 6, 标, 书, 。, 地, 政, 总, 署, ...   \n",
       "\n",
       "                                                 ids  \\\n",
       "0  [101, 4960, 1139, 4906, 772, 6084, 1394, 117, ...   \n",
       "1  [101, 3295, 1744, 1310, 1358, 6913, 1726, 3678...   \n",
       "2  [101, 1159, 3635, 677, 2356, 4509, 6435, 3152,...   \n",
       "3  [101, 3173, 3209, 704, 1744, 113, 121, 123, 12...   \n",
       "4  [101, 1818, 1914, 1164, 6887, 1765, 4649, 2970...   \n",
       "\n",
       "                                               masks  \n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4554, 5)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_output = pd.read_pickle(df_output_path)\n",
    "df_output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4554, 25)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all = pd.merge(df_test, df_output, on='docid', how='left')\n",
    "df_all['input_len'] = df_all[\"input_text\"].str.len()\n",
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>docid</th>\n",
       "      <th>headline</th>\n",
       "      <th>content</th>\n",
       "      <th>ner</th>\n",
       "      <th>matched_keywords</th>\n",
       "      <th>include_company</th>\n",
       "      <th>include_person</th>\n",
       "      <th>length</th>\n",
       "      <th>split_sentence_index</th>\n",
       "      <th>Companys</th>\n",
       "      <th>...</th>\n",
       "      <th>sign</th>\n",
       "      <th>input_text</th>\n",
       "      <th>tokenized_content_x</th>\n",
       "      <th>ids</th>\n",
       "      <th>masks</th>\n",
       "      <th>tokenized_content_y</th>\n",
       "      <th>label_pred</th>\n",
       "      <th>Company_pred</th>\n",
       "      <th>Person_pred</th>\n",
       "      <th>input_len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2023063021774796605&amp;&amp;2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '21...</td>\n",
       "      <td>['黄如']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>471</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...</td>\n",
       "      <td>[突, 出, 科, 产, 融, 合, ,, 推, 动, 创, 新, 链, 、, 产, 业, ...</td>\n",
       "      <td>[101, 4960, 1139, 4906, 772, 6084, 1394, 117, ...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[突, 出, 科, 产, 融, 合, ,, 推, 动, 创, 新, 链, 、, 产, 业, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[黄如]</td>\n",
       "      <td>471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023063021774797155&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>曾国卫受邀回母校　鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校...</td>\n",
       "      <td>[{'label_name': 'Person', 'text_segment': '曾國衞...</td>\n",
       "      <td>['长和', '曾国卫', '赵霁']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>['长和']</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>曾国卫受邀回母校鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校东...</td>\n",
       "      <td>[曾, 国, 卫, 受, 邀, 回, 母, 校, 鼓, 励, 同, 学, 深, 入, 认, ...</td>\n",
       "      <td>[101, 3295, 1744, 1310, 1358, 6913, 1726, 3678...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[曾, 国, 卫, 受, 邀, 回, 母, 校, 鼓, 励, 同, 学, 深, 入, 认, ...</td>\n",
       "      <td>[B-Person, I-Person, I-Person, O, O, O, O, O, ...</td>\n",
       "      <td>[steamroom, 东莞同乡会方树泉学校]</td>\n",
       "      <td>[赵霁, 曾国卫]</td>\n",
       "      <td>495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2023063021774797862&amp;&amp;1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由A轮的每股0.1...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '黑芝...</td>\n",
       "      <td>['汽车', '小米', 'SoC', '中国银行', '腾讯', '黑芝麻集资', 'Oc...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>482</td>\n",
       "      <td>1</td>\n",
       "      <td>['小米', '黑芝', 'Oceanpine', '北极光创投', '汽车', '黑芝麻'...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由a轮的每股0.1...</td>\n",
       "      <td>[初, 步, 上, 市, 申, 请, 文, 件, 披, 露, ，, 黑, 芝, 麻, 2, ...</td>\n",
       "      <td>[101, 1159, 3635, 677, 2356, 4509, 6435, 3152,...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[初, 步, 上, 市, 申, 请, 文, 件, 披, 露, ，, 黑, 芝, 麻, 2, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, B-Company, I...</td>\n",
       "      <td>[黑芝麻, 中国银行, 北极光创投, 小米, 汽车, 吉利集团, 腾讯]</td>\n",
       "      <td>[]</td>\n",
       "      <td>482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2023063021774798050&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '新明...</td>\n",
       "      <td>['新明中国']</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>55</td>\n",
       "      <td>0</td>\n",
       "      <td>['新明中国']</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...</td>\n",
       "      <td>[新, 明, 中, 国, (, 0, 2, 6, 9, 9, ), 内, 幕, 消, 息, ...</td>\n",
       "      <td>[101, 3173, 3209, 704, 1744, 113, 121, 123, 12...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[新, 明, 中, 国, (, 0, 2, 6, 9, 9, ), 内, 幕, 消, 息, ...</td>\n",
       "      <td>[B-Company, I-Company, I-Company, I-Company, O...</td>\n",
       "      <td>[新明中国]</td>\n",
       "      <td>[]</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2023063021774805645&amp;&amp;0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...</td>\n",
       "      <td>[{'label_name': 'Company', 'text_segment': '地政...</td>\n",
       "      <td>['新地', '长实', '会德丰地产', '信置', '嘉华国际', '交通', '招商局...</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>293</td>\n",
       "      <td>0</td>\n",
       "      <td>['新地', '长实', '会德丰地产', '信置', '嘉华国际', '交通', '招商局...</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...</td>\n",
       "      <td>[域, 多, 利, 道, 地, 皮, 接, 6, 标, 书, 。, 地, 政, 总, 署, ...</td>\n",
       "      <td>[101, 1818, 1914, 1164, 6887, 1765, 4649, 2970...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>[域, 多, 利, 道, 地, 皮, 接, 6, 标, 书, 。, 地, 政, 总, 署, ...</td>\n",
       "      <td>[O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...</td>\n",
       "      <td>[交通, 鹰君, 新地, 长实, 巴士, 会德丰地产, 嘉华国际, 信置, 招商局置地]</td>\n",
       "      <td>[]</td>\n",
       "      <td>293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    docid  headline  \\\n",
       "0  2023063021774796605&&2       NaN   \n",
       "1  2023063021774797155&&0       NaN   \n",
       "2  2023063021774797862&&1       NaN   \n",
       "3  2023063021774798050&&0       NaN   \n",
       "4  2023063021774805645&&0       NaN   \n",
       "\n",
       "                                             content  \\\n",
       "0  突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...   \n",
       "1  曾国卫受邀回母校　鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校...   \n",
       "2  初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由A轮的每股0.1...   \n",
       "3  新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...   \n",
       "4  域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...   \n",
       "\n",
       "                                                 ner  \\\n",
       "0  [{'label_name': 'Company', 'text_segment': '21...   \n",
       "1  [{'label_name': 'Person', 'text_segment': '曾國衞...   \n",
       "2  [{'label_name': 'Company', 'text_segment': '黑芝...   \n",
       "3  [{'label_name': 'Company', 'text_segment': '新明...   \n",
       "4  [{'label_name': 'Company', 'text_segment': '地政...   \n",
       "\n",
       "                                    matched_keywords  include_company  \\\n",
       "0                                             ['黄如']             True   \n",
       "1                                ['长和', '曾国卫', '赵霁']             True   \n",
       "2  ['汽车', '小米', 'SoC', '中国银行', '腾讯', '黑芝麻集资', 'Oc...             True   \n",
       "3                                           ['新明中国']             True   \n",
       "4  ['新地', '长实', '会德丰地产', '信置', '嘉华国际', '交通', '招商局...             True   \n",
       "\n",
       "   include_person  length  split_sentence_index  \\\n",
       "0            True     471                     2   \n",
       "1            True     496                     0   \n",
       "2            True     482                     1   \n",
       "3            True      55                     0   \n",
       "4            True     293                     0   \n",
       "\n",
       "                                            Companys  ...   sign  \\\n",
       "0                                                 []  ...  False   \n",
       "1                                             ['长和']  ...  False   \n",
       "2  ['小米', '黑芝', 'Oceanpine', '北极光创投', '汽车', '黑芝麻'...  ...  False   \n",
       "3                                           ['新明中国']  ...  False   \n",
       "4  ['新地', '长实', '会德丰地产', '信置', '嘉华国际', '交通', '招商局...  ...  False   \n",
       "\n",
       "                                          input_text  \\\n",
       "0  突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题...   \n",
       "1  曾国卫受邀回母校鼓励同学深入认识祖国。政制及内地事务局局长曾国卫今日(30日)受邀回到母校东...   \n",
       "2  初步上市申请文件披露，黑芝麻2016年成立，上市前已进行10轮融资，入股价由a轮的每股0.1...   \n",
       "3  新明中国(02699) 内幕消息 / 停牌。新明中国(02699) 复牌进度的季度更新(30...   \n",
       "4  域多利道地皮接6标书。地政总署公布，坚尼地城西宁街与域多利道交界卖地表地皮(下称域多利道地皮...   \n",
       "\n",
       "                                 tokenized_content_x  \\\n",
       "0  [突, 出, 科, 产, 融, 合, ,, 推, 动, 创, 新, 链, 、, 产, 业, ...   \n",
       "1  [曾, 国, 卫, 受, 邀, 回, 母, 校, 鼓, 励, 同, 学, 深, 入, 认, ...   \n",
       "2  [初, 步, 上, 市, 申, 请, 文, 件, 披, 露, ，, 黑, 芝, 麻, 2, ...   \n",
       "3  [新, 明, 中, 国, (, 0, 2, 6, 9, 9, ), 内, 幕, 消, 息, ...   \n",
       "4  [域, 多, 利, 道, 地, 皮, 接, 6, 标, 书, 。, 地, 政, 总, 署, ...   \n",
       "\n",
       "                                                 ids  \\\n",
       "0  [101, 4960, 1139, 4906, 772, 6084, 1394, 117, ...   \n",
       "1  [101, 3295, 1744, 1310, 1358, 6913, 1726, 3678...   \n",
       "2  [101, 1159, 3635, 677, 2356, 4509, 6435, 3152,...   \n",
       "3  [101, 3173, 3209, 704, 1744, 113, 121, 123, 12...   \n",
       "4  [101, 1818, 1914, 1164, 6887, 1765, 4649, 2970...   \n",
       "\n",
       "                                               masks  \\\n",
       "0  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "1  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "2  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "3  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "4  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...   \n",
       "\n",
       "                                 tokenized_content_y  \\\n",
       "0  [突, 出, 科, 产, 融, 合, ,, 推, 动, 创, 新, 链, 、, 产, 业, ...   \n",
       "1  [曾, 国, 卫, 受, 邀, 回, 母, 校, 鼓, 励, 同, 学, 深, 入, 认, ...   \n",
       "2  [初, 步, 上, 市, 申, 请, 文, 件, 披, 露, ，, 黑, 芝, 麻, 2, ...   \n",
       "3  [新, 明, 中, 国, (, 0, 2, 6, 9, 9, ), 内, 幕, 消, 息, ...   \n",
       "4  [域, 多, 利, 道, 地, 皮, 接, 6, 标, 书, 。, 地, 政, 总, 署, ...   \n",
       "\n",
       "                                          label_pred  \\\n",
       "0  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "1  [B-Person, I-Person, I-Person, O, O, O, O, O, ...   \n",
       "2  [O, O, O, O, O, O, O, O, O, O, O, B-Company, I...   \n",
       "3  [B-Company, I-Company, I-Company, I-Company, O...   \n",
       "4  [O, O, O, O, O, O, O, O, O, O, O, O, O, O, O, ...   \n",
       "\n",
       "                                   Company_pred Person_pred input_len  \n",
       "0                                            []        [黄如]       471  \n",
       "1                       [steamroom, 东莞同乡会方树泉学校]   [赵霁, 曾国卫]       495  \n",
       "2          [黑芝麻, 中国银行, 北极光创投, 小米, 汽车, 吉利集团, 腾讯]          []       482  \n",
       "3                                        [新明中国]          []        55  \n",
       "4  [交通, 鹰君, 新地, 长实, 巴士, 会德丰地产, 嘉华国际, 信置, 招商局置地]          []       293  \n",
       "\n",
       "[5 rows x 25 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_all.to_excel(df_save_path, index=False, engine='xlsxwriter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['docid', 'headline', 'content', 'ner', 'matched_keywords',\n",
       "       'include_company', 'include_person', 'length', 'split_sentence_index',\n",
       "       'Companys', 'Persons', 'context_cleaned', 'person_matched',\n",
       "       'company_matched', 'context_keywords', 'sign', 'input_text',\n",
       "       'tokenized_content_x', 'ids', 'masks', 'tokenized_content_y',\n",
       "       'label_pred', 'Company_pred', 'Person_pred', 'input_len'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from zhconv import convert\n",
    "df_all['content'] = df_all['content'].apply(lambda content: convert(content, 'zh-cn'))\n",
    "df_all.sample(200, random_state=2)[['docid', 'content', \"person_matched\", \"company_matched\", 'Company_pred', 'Person_pred']].to_excel(\"/workspace/NER/testset_sample.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from evalner import partial_match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_all = df_all[df_all['input_len'] <= 510]\n",
    "\n",
    "df_all.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4554, 25)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def findBoundary(val, text):\n",
    "    # 寻找抽取到的实体的边界索引\n",
    "    res = []\n",
    "    for i in range(0, len(text) - len(val) + 1):\n",
    "        if text[i:i + len(val)] == val:\n",
    "            res.append((i, i + len(val)))\n",
    "    return res\n",
    "\n",
    "def checkIfOverlap(pred_val, true_val, text):\n",
    "    # 检查两个实体是否部分匹配，该方法中intersec是判断两个实体的索引是否有交集，有交集即为True, 还有严格的计算标准就是是否完全匹配\n",
    "    rang_a = findBoundary(true_val, text)\n",
    "    rang_b = findBoundary(pred_val, text)\n",
    "    # print(rang_a, rang_b)\n",
    "    if len(rang_a) == 0 or len(rang_b) == 0:\n",
    "        return False\n",
    "    else:\n",
    "        for i, j in rang_a:\n",
    "            for k, m in rang_b:\n",
    "                if (i, j) == (k, m):\n",
    "                    return True\n",
    "                else:\n",
    "                    return False\n",
    "                # 计算两个实体的索引是否有交集，如果有交集即是实体部分匹配\n",
    "                # intersec = set(range(i, j)).intersection(set(range(k, m)))\n",
    "                # if len(intersec) > 0:\n",
    "                #     return True\n",
    "                # else:\n",
    "                #     return False\n",
    "                \n",
    "def partial_match(pred_list, golden_list, text):\n",
    "    precision_hit, precision_all = 0, 0\n",
    "    recall_hit, recall_all = 0, 0\n",
    "    \n",
    "    for pred in pred_list:\n",
    "        precision_all += 1\n",
    "        for golden in golden_list:\n",
    "            if checkIfOverlap(pred, golden, text):\n",
    "                precision_hit += 1\n",
    "                break\n",
    "    \n",
    "    for golden in golden_list:\n",
    "        recall_all += 1\n",
    "        for pred in pred_list:\n",
    "            if checkIfOverlap(pred, golden, text):\n",
    "                recall_hit += 1\n",
    "                break\n",
    "\n",
    "    return precision_hit, precision_all, recall_hit, recall_all\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:16<00:00,  8.27s/it]\n"
     ]
    }
   ],
   "source": [
    "result = {}\n",
    "for entity_type in tqdm(args.entity_type):\n",
    "    p_hit, p_all, r_hit, r_all = 0, 0, 0, 0\n",
    "    for _, row in df_all.iterrows():\n",
    "        # print(row)\n",
    "        golden_list = row[f'{entity_type}_matched'.lower()] ##\n",
    "        pred_list = row[f'{entity_type}_pred'] ##\n",
    "        # text = str(row['context_cleaned']) ##\n",
    "        text = str(row['input_text']) ##\n",
    "        # print(golden_list, pred_list, text)\n",
    "        p_hit_, p_all_, r_hit_, r_all_ = partial_match(pred_list, golden_list, text)\n",
    "        p_hit += p_hit_\n",
    "        p_all += p_all_\n",
    "        r_hit += r_hit_\n",
    "        r_all += r_all_\n",
    "    \n",
    "    p = p_hit / p_all\n",
    "    r = r_hit / r_all\n",
    "    \n",
    "    result.update(\n",
    "        {\n",
    "            f\"{entity_type}\": {\n",
    "                \"precision\": round(p, 4),\n",
    "                \"recall\": round(r, 4),\n",
    "                \"f1\": round(2 * p * r / (p + r), 4),\n",
    "                \"details\": [p_hit, p_all, r_hit, r_all]\n",
    "            }\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Company': {'precision': 0.796,\n",
       "  'recall': 0.7355,\n",
       "  'f1': 0.7645,\n",
       "  'details': [10267, 12899, 10267, 13960]},\n",
       " 'Person': {'precision': 0.8643,\n",
       "  'recall': 0.8446,\n",
       "  'f1': 0.8543,\n",
       "  'details': [4912, 5683, 4912, 5816]}}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: fuzzywuzzy==0.18.0 in /opt/conda/lib/python3.8/site-packages (0.18.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install fuzzywuzzy==0.18.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from muc import evaluate_all\n",
    "\n",
    "def run_eval4ner(df, entity_type):\n",
    "    # docid_remove = [df_row['docid'] for _, df_row in df.iterrows() if (df_row[f'{entity_type}_pred'] == []) or (df_row[\"input_len\"] > 510)] #  and (df_row[f'{entity_type}_pred'] == [])(df_row[f'{entity_type}_gt'] == [])\n",
    "    # df_eval = df[~df['docid'].isin(docid_remove)]\n",
    "    df_eval = df\n",
    "    print(\"Number of data for evaluation:\", len(df_eval))\n",
    "\n",
    "    pred = [[(f\"{entity_type}\", j.lower()) for j in i] for i in df_eval[f'{entity_type}_pred'].to_list()]\n",
    "    label_gt = [[(f\"{entity_type}\", j.lower()) for j in i] for i in df_eval[f'{entity_type}_matched'.lower()].to_list()]\n",
    "     \n",
    "    print(f\"======= {entity_type} =======\\nExample:\\ncontent:{df_eval['content'].tolist()[0]},\\npred: {pred[0]}\\nlabel: {label_gt[0]}\")\n",
    "    evaluate_all(pred, label_gt, texts=df_eval['input_text'].to_list())\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data for evaluation: 4554\n",
      "======= Company =======\n",
      "Example:\n",
      "content:突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题,聚力产出一批高水平标志性科技成果,更好带动产业集聚集群发展。突出行业融合,积极探索新型会员模式,开展共性技术难题攻关,强化行业内部创新资源融合共享,形成多方共建共治共享的治理结构。希望国创中心加快实体化运作,在理事会领导下高效执行、快速起跑,全力推动各项工作有序开展,真正把国创中心建设成为集成电路设计自动化产业的技术发源地、产品示范地和人才汇聚地。东南大学校长黄如表示,要努力将EDA国创中心打造成国内领先、世界一流的EDA技术创新中心、人才培养基地和产业孵化重镇。以揭牌为起点,未来中心将提高站位,坚持\"全国创新中心\"核心定位,充分整合国内外优质资源,构建高校领衔、企业加盟、产学研深度融合的新型科技攻关体制,打通EDA领域上下游合作机制,形成全国一盘棋、全球一张网的科技研发力量,合力打造EDA\"国家队\"。围绕国家战略和产业需求,要坚持近期目标与长远目标相结合、当前技术与下一代技术相结合的原则,科学制定工作规划和项目目标,尽快落地标志性成果,\n",
      "pred: []\n",
      "label: []\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7769, Recall=0.7468, F1:0.7430\n",
      "   exact mode, Precision=0.7769, Recall=0.7468, F1:0.7430\n",
      " partial mode, Precision=0.8493, Recall=0.8062, F1:0.8075\n",
      "    type mode, Precision=0.8493, Recall=0.8062, F1:0.8075\n",
      " similar mode, Precision=0.8515, Recall=0.8079, F1:0.8096\n",
      "\n",
      "\n",
      "\n",
      "Number of data for evaluation: 4554\n",
      "======= Person =======\n",
      "Example:\n",
      "content:突出科产融合,推动创新链、产业链、资金链、人才链深度融合,着力解决一批基础理论和技术原理问题,聚力产出一批高水平标志性科技成果,更好带动产业集聚集群发展。突出行业融合,积极探索新型会员模式,开展共性技术难题攻关,强化行业内部创新资源融合共享,形成多方共建共治共享的治理结构。希望国创中心加快实体化运作,在理事会领导下高效执行、快速起跑,全力推动各项工作有序开展,真正把国创中心建设成为集成电路设计自动化产业的技术发源地、产品示范地和人才汇聚地。东南大学校长黄如表示,要努力将EDA国创中心打造成国内领先、世界一流的EDA技术创新中心、人才培养基地和产业孵化重镇。以揭牌为起点,未来中心将提高站位,坚持\"全国创新中心\"核心定位,充分整合国内外优质资源,构建高校领衔、企业加盟、产学研深度融合的新型科技攻关体制,打通EDA领域上下游合作机制,形成全国一盘棋、全球一张网的科技研发力量,合力打造EDA\"国家队\"。围绕国家战略和产业需求,要坚持近期目标与长远目标相结合、当前技术与下一代技术相结合的原则,科学制定工作规划和项目目标,尽快落地标志性成果,\n",
      "pred: [('Person', '黄如')]\n",
      "label: [('Person', '黄如')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8818, Recall=0.8813, F1:0.8756\n",
      "   exact mode, Precision=0.8818, Recall=0.8813, F1:0.8756\n",
      " partial mode, Precision=0.8918, Recall=0.8901, F1:0.8849\n",
      "    type mode, Precision=0.8918, Recall=0.8901, F1:0.8849\n",
      " similar mode, Precision=0.8955, Recall=0.8937, F1:0.8885\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entity_type in args.entity_type:\n",
    "    run_eval4ner(df_all, entity_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from muc import evaluate_all\n",
    "\n",
    "def run_eval4ner(df, entity_type):\n",
    "    docid_remove_gt = [df_row['docid'] for _, df_row in df.iterrows() if (df_row[f'{entity_type}_matched'.lower()] == []) or (df_row[\"input_len\"] > 510)] #  and (df_row[f'{entity_type}_pred'] == [])(df_row[f'{entity_type}_gt'] == [])\n",
    "  \n",
    "    df_eval_gt = df[~df['docid'].isin(docid_remove_gt)]\n",
    " \n",
    "    print(\"Number of data for evaluation:\", len(df_eval_gt))\n",
    " \n",
    "    pred_gpt = [[(f\"{entity_type}\", j.lower()) for j in i] for i in df_eval_gt[f'{entity_type}_pred'].to_list()]\n",
    "    label_gpt = [[(f\"{entity_type}\", j.lower()) for j in i] for i in df_eval_gt[f'{entity_type}_matched'.lower()].to_list()]\n",
    "   \n",
    "    print(f\"======= {entity_type} =======\\nExample:\\npred: {pred_gpt[0]}\\nGPT: {label_gpt[0]}\")\n",
    "    evaluate_all(pred_gpt, label_gpt, texts=df_eval_gt['input_text'].to_list())\n",
    "\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data for evaluation: 4132\n",
      "======= Company =======\n",
      "Example:\n",
      "pred: [('Company', 'steamroom'), ('Company', '东莞同乡会方树泉学校')]\n",
      "GPT: [('Company', '长和')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7734, Recall=0.7406, F1:0.7363\n",
      "   exact mode, Precision=0.7734, Recall=0.7406, F1:0.7363\n",
      " partial mode, Precision=0.8528, Recall=0.8057, F1:0.8071\n",
      "    type mode, Precision=0.8528, Recall=0.8057, F1:0.8071\n",
      " similar mode, Precision=0.8553, Recall=0.8077, F1:0.8094\n",
      "\n",
      "\n",
      "\n",
      "Number of data for evaluation: 2469\n",
      "======= Person =======\n",
      "Example:\n",
      "pred: [('Person', '黄如')]\n",
      "GPT: [('Person', '黄如')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8391, Recall=0.8382, F1:0.8277\n",
      "   exact mode, Precision=0.8391, Recall=0.8382, F1:0.8277\n",
      " partial mode, Precision=0.8575, Recall=0.8544, F1:0.8448\n",
      "    type mode, Precision=0.8575, Recall=0.8544, F1:0.8448\n",
      " similar mode, Precision=0.8644, Recall=0.8611, F1:0.8515\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for entity_type in args.entity_type:\n",
    "    run_eval4ner(df_all, entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9610486239009417"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_f1(p, r):\n",
    "    return 2*p*r / (p + r)\n",
    "\n",
    "compute_f1(0.9599, 0.9622)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 对badcase数据集进行当前模型的测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "badcase_data = pd.read_excel(\"/workspace/client_project/poc/bochk/badcase/bochk_uat_20240410_ner.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'doc_id': 2023120100000939360,\n",
       "  'headline': '英再有城市破產 諾定咸超支2.3億',\n",
       "  'content': '\\n繼英國第二大城市伯明翰市議會9月初宣布破產後，英國中部城市諾定咸（Nottingham）市議會也於周三（29日）宣布破產，當局歸因於福利需求上升及通脹等全國性因素。\\n\\n事實上，市議會在理財上一直為人詬病，\\n\\n...\\n\\n訂閱hket.com即送 GNC男士綜合維他命加強版 (價值$268)\\n\\n詳情\\n\\n登入了解詳情\\n',\n",
       "  'ner_label_type': 'Company',\n",
       "  'ner_keyword': 'Nottingham'},\n",
       " {'doc_id': 2023120100001090313,\n",
       "  'headline': '中央谷長三角 寧滬盈利復增',\n",
       "  'content': '  江蘇寧滬高速（00177）主業務是收費路橋的投資、建設、營運和管理，經營區域在長江三角洲，是江蘇省唯一上市路橋公司。而公司盈利經過2022年倒退後，今年可以恢復增長，中央重申推動長江經濟帶高質量發展，   訂閱hket.com即送 GNC男士綜合維他命加強版 (價值$268) 詳情 \\n\\n 登入 \\n\\n 了解詳情 \\n\\n 撰文 : 余少文 \\n\\n 欄名 : 中字股實力談\\n',\n",
       "  'ner_label_type': 'Company',\n",
       "  'ner_keyword': '江蘇寧滬高速'}]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "badcase_data.head(2).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cp /workspace/NER/experiments/0606_bert_p\\=0.5_pos\\=sentence_BP/model.pth /workspace/ner-starbucks/src/models/model_0606.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用新接口对测试集进行ner打标并统计结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4554, 20)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_62410/1095725994.py:3: FutureWarning: Using short name for 'orient' is deprecated. Only the options: ('dict', list, 'series', 'split', 'records', 'index') will be used in a future version. Use one of the above to silence this warning.\n",
      "  df_test_list = df_test.to_dict('record')\n",
      "100%|██████████| 4554/4554 [04:04<00:00, 18.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import requests\n",
    "df_test_list = df_test.to_dict('record')\n",
    "# url = \"http://ess76.wisers.com:13352/ner/content\"\n",
    "\n",
    "url = \"http://ess73.wisers.com:13347/ner/content\"\n",
    "ner_tag_result = []\n",
    "for row in tqdm(df_test_list):\n",
    "    input_param = {\n",
    "        \"docid\": row['docid'],\n",
    "        \"content\": row[\"content\"],\n",
    "        \"add_keywords\": False\n",
    "    }\n",
    "    response = requests.post(url, json=input_param)\n",
    "    try:\n",
    "        if response.status_code == 200:\n",
    "            result = response.json()\n",
    "            ner_result = result['retData']\n",
    "            dict_matched_company = []\n",
    "            dict_matched_person = []\n",
    "            algorithm_matched_company = []\n",
    "            algorithm_matched_person = []\n",
    "\n",
    "            for info in ner_result['text']:\n",
    "                if info['label_name'] == 'Company':\n",
    "                    if info['resource'] == 'Algorithm prediction':\n",
    "                        algorithm_matched_company.append(info['text_segment'])\n",
    "                    elif info['resource'] == \"Dictionary matching\":\n",
    "                        dict_matched_company.append(info['text_segment'])\n",
    "                elif info['label_name'] == 'Person':\n",
    "                    if info['resource'] == 'Algorithm prediction':\n",
    "                        algorithm_matched_person.append(info['text_segment'])\n",
    "                    elif info['resource'] == \"Dictionary matching\":\n",
    "                        dict_matched_person.append(info['text_segment'])\n",
    "                        \n",
    "            ner_tag_result.append({\n",
    "                \"docid\": row['docid'],\n",
    "                \"ner_result\": ner_result,\n",
    "                \"dict_matched_company\": dict_matched_company,\n",
    "                \"dict_matched_person\": dict_matched_person,\n",
    "                \"algorithm_matched_company\": algorithm_matched_company,\n",
    "                \"algorithm_matched_person\": algorithm_matched_person,\n",
    "            })\n",
    "    except Exception as e:\n",
    "        print(e, response)\n",
    "\n",
    "                        \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ner_tag_add_origin_data = df_test.merge(pd.DataFrame(ner_tag_result), on='docid', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['docid', 'headline', 'content', 'ner', 'matched_keywords',\n",
       "       'include_company', 'include_person', 'length', 'split_sentence_index',\n",
       "       'Companys', 'Persons', 'context_cleaned', 'person_matched',\n",
       "       'company_matched', 'context_keywords', 'sign', 'input_text',\n",
       "       'tokenized_content', 'ids', 'masks', 'ner_result',\n",
       "       'dict_matched_company', 'dict_matched_person',\n",
       "       'algorithm_matched_company', 'algorithm_matched_person'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ner_tag_add_origin_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ner_tag_add_origin_data[['docid', 'content', 'ner', 'ner_result', 'person_matched', 'company_matched', 'dict_matched_company', 'dict_matched_person',\n",
    "       'algorithm_matched_company', 'algorithm_matched_person']].to_excel(\"test_ner_api_result.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data for evaluation: 4132\n",
      "======= Company =======\n",
      "Example:\n",
      "pred: [('Company', '东莞同乡会方树泉学校'), ('Company', '东莞同乡会方树泉学校'), ('Company', '东莞同乡会方树泉学校'), ('Company', 'steam'), ('Company', '东莞同乡会方树泉学校')]\n",
      "GPT: [('Company', '长和')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7480, Recall=0.7158, F1:0.7104\n",
      "   exact mode, Precision=0.7480, Recall=0.7158, F1:0.7104\n",
      " partial mode, Precision=0.8366, Recall=0.7844, F1:0.7875\n",
      "    type mode, Precision=0.8366, Recall=0.7844, F1:0.7875\n",
      " similar mode, Precision=0.8378, Recall=0.7849, F1:0.7884\n",
      "\n",
      "\n",
      "\n",
      "Number of data for evaluation: 2469\n",
      "======= Person =======\n",
      "Example:\n",
      "pred: [('Person', '黄如')]\n",
      "GPT: [('Person', '黄如')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8051, Recall=0.8057, F1:0.7933\n",
      "   exact mode, Precision=0.8051, Recall=0.8057, F1:0.7933\n",
      " partial mode, Precision=0.8324, Recall=0.8288, F1:0.8182\n",
      "    type mode, Precision=0.8324, Recall=0.8288, F1:0.8182\n",
      " similar mode, Precision=0.8327, Recall=0.8289, F1:0.8185\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner_tag_add_origin_data[\"Company_pred\"] = ner_tag_add_origin_data['algorithm_matched_company']\n",
    "ner_tag_add_origin_data[\"Person_pred\"] = ner_tag_add_origin_data['algorithm_matched_person']\n",
    "ner_tag_add_origin_data['input_len'] = ner_tag_add_origin_data['content'].apply(len)\n",
    "for entity_type in args.entity_type:\n",
    "    run_eval4ner(ner_tag_add_origin_data, entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data for evaluation: 4132\n",
      "======= Company =======\n",
      "Example:\n",
      "pred: [('Company', '东莞同乡会方树泉学校'), ('Company', '东莞同乡会方树泉学校'), ('Company', '东莞同乡会方树泉学校'), ('Company', 'steam'), ('Company', '东莞同乡会方树泉学校'), ('Company', '政制及内地事务局'), ('Company', '政制及内地事务局')]\n",
      "GPT: [('Company', '长和')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.7436, Recall=0.7775, F1:0.7379\n",
      "   exact mode, Precision=0.7436, Recall=0.7775, F1:0.7379\n",
      " partial mode, Precision=0.8521, Recall=0.8370, F1:0.8237\n",
      "    type mode, Precision=0.8521, Recall=0.8370, F1:0.8237\n",
      " similar mode, Precision=0.8535, Recall=0.8375, F1:0.8248\n",
      "\n",
      "\n",
      "\n",
      "Number of data for evaluation: 2469\n",
      "======= Person =======\n",
      "Example:\n",
      "pred: [('Person', '黄如')]\n",
      "GPT: [('Person', '黄如')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.8051, Recall=0.8057, F1:0.7933\n",
      "   exact mode, Precision=0.8051, Recall=0.8057, F1:0.7933\n",
      " partial mode, Precision=0.8324, Recall=0.8288, F1:0.8182\n",
      "    type mode, Precision=0.8324, Recall=0.8288, F1:0.8182\n",
      " similar mode, Precision=0.8327, Recall=0.8289, F1:0.8185\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner_tag_add_origin_data[\"Company_pred\"] = ner_tag_add_origin_data.apply(lambda row: row['algorithm_matched_company'] + row['dict_matched_company'], axis=1)\n",
    "ner_tag_add_origin_data[\"Person_pred\"] = ner_tag_add_origin_data.apply(lambda row: row['algorithm_matched_person'] + row['dict_matched_person'], axis=1)\n",
    "\n",
    "for entity_type in args.entity_type:\n",
    "    run_eval4ner(ner_tag_add_origin_data, entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data for evaluation: 4132\n",
      "======= Company =======\n",
      "Example:\n",
      "pred: [('Company', '政制及内地事务局'), ('Company', '政制及内地事务局')]\n",
      "GPT: [('Company', '长和')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.1276, Recall=0.0617, F1:0.0745\n",
      "   exact mode, Precision=0.1276, Recall=0.0617, F1:0.0745\n",
      " partial mode, Precision=0.2350, Recall=0.1082, F1:0.1350\n",
      "    type mode, Precision=0.2350, Recall=0.1082, F1:0.1350\n",
      " similar mode, Precision=0.2358, Recall=0.1084, F1:0.1354\n",
      "\n",
      "\n",
      "\n",
      "Number of data for evaluation: 2469\n",
      "======= Person =======\n",
      "Example:\n",
      "pred: []\n",
      "GPT: [('Person', '黄如')]\n",
      "\n",
      " NER evaluation scores:\n",
      "  strict mode, Precision=0.0000, Recall=0.0000, F1:0.0000\n",
      "   exact mode, Precision=0.0000, Recall=0.0000, F1:0.0000\n",
      " partial mode, Precision=0.0000, Recall=0.0000, F1:0.0000\n",
      "    type mode, Precision=0.0000, Recall=0.0000, F1:0.0000\n",
      " similar mode, Precision=0.0000, Recall=0.0000, F1:0.0000\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ner_tag_add_origin_data[\"Company_pred\"] = ner_tag_add_origin_data['dict_matched_company']\n",
    "ner_tag_add_origin_data[\"Person_pred\"] = ner_tag_add_origin_data['dict_matched_person']\n",
    "for entity_type in args.entity_type:\n",
    "    run_eval4ner(ner_tag_add_origin_data, entity_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000.0"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
